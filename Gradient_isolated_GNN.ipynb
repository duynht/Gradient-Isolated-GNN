{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient-isolated GNN",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duynht/Greedy_InfoMax/blob/master/Gradient_isolated_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y6NeVTYs1Y5",
        "colab_type": "text"
      },
      "source": [
        "# README\n",
        "This notebook houses preliminary experiments for applying gradient-isolated training to Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucy5n13wO1vn",
        "colab_type": "text"
      },
      "source": [
        "# Keep Colab running in the background\n",
        "Set a JavaScript interval to click on the connect button every 60 seconds. Open developer-settings (in your web-browser) with Ctrl+Shift+I then click on console tab and paste this to the console prompt. (for Mac press Option+Command+I)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "<!-- function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000); -->\n",
        "```\n",
        "Little did I know Colab changed their selector id with this fancy update so we'll use the now code then. This time we have proper start and stop function. LOLOLOLOLOL\n",
        "\n",
        "```\n",
        "var startClickConnect = function startClickConnect(){\n",
        "    var clickConnect = function clickConnect(){\n",
        "        console.log(\"Connnect Clicked - Start\");\n",
        "        document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "        console.log(\"Connnect Clicked - End\"); \n",
        "    };\n",
        "\n",
        "    var intervalId = setInterval(clickConnect, 60000);\n",
        "\n",
        "    var stopClickConnectHandler = function stopClickConnect() {\n",
        "        console.log(\"Connnect Clicked Stopped - Start\");\n",
        "        clearInterval(intervalId);\n",
        "        console.log(\"Connnect Clicked Stopped - End\");\n",
        "    };\n",
        "\n",
        "    return stopClickConnectHandler;\n",
        "};\n",
        "\n",
        "var stopClickConnect = startClickConnect();\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55OhTMtw966A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzvtYMM4_GLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/duynht/Greedy_InfoMax.git\n",
        "%cd /content/Greedy_InfoMax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isWyIDA7tVty",
        "colab_type": "text"
      },
      "source": [
        "# Graph package installation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cs99CJQtZka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torchvision==0.5.0\n",
        "!pip install torchaudio==0.4.0\n",
        "!pip install torch==1.4.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvLyaZyjteVO",
        "colab_type": "text"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4XAYrLutQF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as npnpinpi\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTAMw8TucquJ",
        "colab_type": "code",
        "outputId": "440523dc-9d47-4493-e72e-144ddf1c87ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from GreedyInfoMax.vision.data import get_dataloader\n",
        "from GreedyInfoMax.vision.arg_parser import arg_parser\n",
        "from GreedyInfoMax.vision.models import load_vision_model, SmallModel\n",
        "from GreedyInfoMax.utils import logger, utils"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyHH0hay_c4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path as osp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afaE0rX7zBw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "# breakpoint() # not working for python 3.6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqrglE3UhD1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "if '-f' in sys.argv:\n",
        "  sys.argv.remove('-f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cj4FDH6RONA",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9a5ZYblcTKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python -m GreedyInfoMax.vision.modded_main_vision \\\n",
        "# --grid_dims 4 \\\n",
        "# --resnet 34 \\\n",
        "# --download_dataset \\\n",
        "# --save_dir \"/content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\" \\\n",
        "# --prediction_step 6 \\\n",
        "# --num_epochs 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TDnUQuGRQi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d137d308-f157-4a4f-b2d0-ac53a0b1f78a"
      },
      "source": [
        "!python -m GreedyInfoMax.vision.modded_downstream_classification \\\n",
        "--grid_dims 4 \\\n",
        "--resnet 34 \\\n",
        "--model_path \"/content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\" \\\n",
        "--model_num 2 \\\n",
        "--num_epochs 200 \\\n",
        "--download_dataset"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (2): PreActBlockNoBN(\n",
            "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./datasets/stl10_binary/stl10_binary.tar.gz\n",
            "100% 2639003648/2640397119 [03:19<00:00, 19389081.55it/s]Extracting ./datasets/stl10_binary/stl10_binary.tar.gz to ./datasets/stl10_binary\n",
            "2640404480it [03:30, 19389081.55it/s]                    tcmalloc: large alloc 2764800000 bytes == 0x5141a000 @  0x7f508c3951e7 0x7f5089bf95e1 0x7f5089c6290d 0x7f5089c63522 0x7f5089cfabce 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509042 0x594931 0x549e5f 0x5513d1 0x5a9cbc 0x50a5c3 0x50cd96 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x516345 0x50a2bf 0x50bfb4 0x507d64 0x509a90\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Use (train+val) / test split\n",
            "Sequential(\n",
            "  (layer1): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "Epoch [1/200], Step [0/157], Time (s): 0.9, Acc1: 15.6250, Acc5: 68.7500, Loss: 2.2441\n",
            "Epoch [1/200], Step [10/157], Time (s): 1.3, Acc1: 15.6250, Acc5: 62.5000, Loss: 2.2719\n",
            "Epoch [1/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 2.1325\n",
            "Epoch [1/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 2.1034\n",
            "Epoch [1/200], Step [40/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 65.6250, Loss: 2.1642\n",
            "Epoch [1/200], Step [50/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 71.8750, Loss: 2.1346\n",
            "Epoch [1/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 2.0040\n",
            "Epoch [1/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9431\n",
            "Epoch [1/200], Step [80/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 2.1076\n",
            "Epoch [1/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9605\n",
            "Epoch [1/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 2.0383\n",
            "Epoch [1/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 2.0081\n",
            "Epoch [1/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.1696\n",
            "Epoch [1/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 2.0587\n",
            "Epoch [1/200], Step [140/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 2.0728\n",
            "Epoch [1/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8842\n",
            "Overall accuracy for this epoch:  23.36783439490446\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "not enough models there yet, nothing to delete\n",
            "not enough models there yet, nothing to delete\n",
            "Epoch [2/200], Step [0/157], Time (s): 2.0, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8559\n",
            "Epoch [2/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8664\n",
            "Epoch [2/200], Step [20/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.9880\n",
            "Epoch [2/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 2.0047\n",
            "Epoch [2/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8432\n",
            "Epoch [2/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0778\n",
            "Epoch [2/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.8208\n",
            "Epoch [2/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7617\n",
            "Epoch [2/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9912\n",
            "Epoch [2/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.9247\n",
            "Epoch [2/200], Step [100/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 75.0000, Loss: 2.1421\n",
            "Epoch [2/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0231\n",
            "Epoch [2/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8259\n",
            "Epoch [2/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.1201\n",
            "Epoch [2/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9421\n",
            "Epoch [2/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8317\n",
            "Overall accuracy for this epoch:  28.343949044585987\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [3/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8870\n",
            "Epoch [3/200], Step [10/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.8394\n",
            "Epoch [3/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.0978\n",
            "Epoch [3/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9281\n",
            "Epoch [3/200], Step [40/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 62.5000, Loss: 2.1953\n",
            "Epoch [3/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 90.6250, Loss: 1.7479\n",
            "Epoch [3/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 2.0726\n",
            "Epoch [3/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8928\n",
            "Epoch [3/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 2.1281\n",
            "Epoch [3/200], Step [90/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 78.1250, Loss: 1.8625\n",
            "Epoch [3/200], Step [100/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.6576\n",
            "Epoch [3/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.9907\n",
            "Epoch [3/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8828\n",
            "Epoch [3/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 1.8695\n",
            "Epoch [3/200], Step [140/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 81.2500, Loss: 1.8956\n",
            "Epoch [3/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7231\n",
            "Overall accuracy for this epoch:  28.463375796178344\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [4/200], Step [0/157], Time (s): 2.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.7386\n",
            "Epoch [4/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.1575\n",
            "Epoch [4/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.9309\n",
            "Epoch [4/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9361\n",
            "Epoch [4/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8499\n",
            "Epoch [4/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8064\n",
            "Epoch [4/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8392\n",
            "Epoch [4/200], Step [70/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 75.0000, Loss: 2.0995\n",
            "Epoch [4/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.7148\n",
            "Epoch [4/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9717\n",
            "Epoch [4/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9043\n",
            "Epoch [4/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 2.0929\n",
            "Epoch [4/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8951\n",
            "Epoch [4/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0764\n",
            "Epoch [4/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7727\n",
            "Epoch [4/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9948\n",
            "Overall accuracy for this epoch:  28.761942675159236\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [5/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7388\n",
            "Epoch [5/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.6660\n",
            "Epoch [5/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 65.6250, Loss: 2.0787\n",
            "Epoch [5/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9754\n",
            "Epoch [5/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7572\n",
            "Epoch [5/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9933\n",
            "Epoch [5/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.7533\n",
            "Epoch [5/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9215\n",
            "Epoch [5/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0186\n",
            "Epoch [5/200], Step [90/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.6498\n",
            "Epoch [5/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 2.0335\n",
            "Epoch [5/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8341\n",
            "Epoch [5/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8503\n",
            "Epoch [5/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8098\n",
            "Epoch [5/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7106\n",
            "Epoch [5/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8841\n",
            "Overall accuracy for this epoch:  30.27468152866242\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [6/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7880\n",
            "Epoch [6/200], Step [10/157], Time (s): 1.1, Acc1: 9.3750, Acc5: 71.8750, Loss: 1.9601\n",
            "Epoch [6/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.7923\n",
            "Epoch [6/200], Step [30/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5723\n",
            "Epoch [6/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8576\n",
            "Epoch [6/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6974\n",
            "Epoch [6/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8199\n",
            "Epoch [6/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 68.7500, Loss: 1.9456\n",
            "Epoch [6/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 1.9432\n",
            "Epoch [6/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0371\n",
            "Epoch [6/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8175\n",
            "Epoch [6/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9934\n",
            "Epoch [6/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 2.0437\n",
            "Epoch [6/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.9186\n",
            "Epoch [6/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8194\n",
            "Epoch [6/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7249\n",
            "Overall accuracy for this epoch:  30.3343949044586\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [7/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6907\n",
            "Epoch [7/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.8997\n",
            "Epoch [7/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7828\n",
            "Epoch [7/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.8122\n",
            "Epoch [7/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7675\n",
            "Epoch [7/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.8217\n",
            "Epoch [7/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8729\n",
            "Epoch [7/200], Step [70/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 2.0654\n",
            "Epoch [7/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8041\n",
            "Epoch [7/200], Step [90/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 59.3750, Loss: 2.3207\n",
            "Epoch [7/200], Step [100/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0786\n",
            "Epoch [7/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7648\n",
            "Epoch [7/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.6194\n",
            "Epoch [7/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6714\n",
            "Epoch [7/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9548\n",
            "Epoch [7/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9375\n",
            "Overall accuracy for this epoch:  29.697452229299362\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [8/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8699\n",
            "Epoch [8/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 96.8750, Loss: 1.7517\n",
            "Epoch [8/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8619\n",
            "Epoch [8/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6968\n",
            "Epoch [8/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8216\n",
            "Epoch [8/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7573\n",
            "Epoch [8/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9400\n",
            "Epoch [8/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7489\n",
            "Epoch [8/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 2.0113\n",
            "Epoch [8/200], Step [90/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.8342\n",
            "Epoch [8/200], Step [100/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0793\n",
            "Epoch [8/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8655\n",
            "Epoch [8/200], Step [120/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 90.6250, Loss: 1.8664\n",
            "Epoch [8/200], Step [130/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 68.7500, Loss: 2.1171\n",
            "Epoch [8/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9056\n",
            "Epoch [8/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9901\n",
            "Overall accuracy for this epoch:  31.369426751592357\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [9/200], Step [0/157], Time (s): 2.0, Acc1: 18.7500, Acc5: 81.2500, Loss: 2.1280\n",
            "Epoch [9/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6300\n",
            "Epoch [9/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9601\n",
            "Epoch [9/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6840\n",
            "Epoch [9/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8196\n",
            "Epoch [9/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0201\n",
            "Epoch [9/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7274\n",
            "Epoch [9/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 1.9694\n",
            "Epoch [9/200], Step [80/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 96.8750, Loss: 1.5627\n",
            "Epoch [9/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7937\n",
            "Epoch [9/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8244\n",
            "Epoch [9/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6834\n",
            "Epoch [9/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8750\n",
            "Epoch [9/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8644\n",
            "Epoch [9/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7606\n",
            "Epoch [9/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6026\n",
            "Overall accuracy for this epoch:  31.488853503184714\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [10/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 68.7500, Loss: 2.0770\n",
            "Epoch [10/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7390\n",
            "Epoch [10/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7591\n",
            "Epoch [10/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7855\n",
            "Epoch [10/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6468\n",
            "Epoch [10/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8417\n",
            "Epoch [10/200], Step [60/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0861\n",
            "Epoch [10/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9488\n",
            "Epoch [10/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8922\n",
            "Epoch [10/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8952\n",
            "Epoch [10/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7889\n",
            "Epoch [10/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.2017\n",
            "Epoch [10/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9313\n",
            "Epoch [10/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7417\n",
            "Epoch [10/200], Step [140/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 93.7500, Loss: 1.7659\n",
            "Epoch [10/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8293\n",
            "Overall accuracy for this epoch:  30.83200636942675\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [11/200], Step [0/157], Time (s): 2.0, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9000\n",
            "Epoch [11/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6627\n",
            "Epoch [11/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8463\n",
            "Epoch [11/200], Step [30/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 68.7500, Loss: 2.2383\n",
            "Epoch [11/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8911\n",
            "Epoch [11/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9322\n",
            "Epoch [11/200], Step [60/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0563\n",
            "Epoch [11/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7241\n",
            "Epoch [11/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 2.0416\n",
            "Epoch [11/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0042\n",
            "Epoch [11/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9580\n",
            "Epoch [11/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 2.1061\n",
            "Epoch [11/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8038\n",
            "Epoch [11/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.9612\n",
            "Epoch [11/200], Step [140/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 84.3750, Loss: 1.6438\n",
            "Epoch [11/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 1.9280\n",
            "Overall accuracy for this epoch:  31.389331210191084\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [12/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8978\n",
            "Epoch [12/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8773\n",
            "Epoch [12/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.9543\n",
            "Epoch [12/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7684\n",
            "Epoch [12/200], Step [40/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0192\n",
            "Epoch [12/200], Step [50/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 81.2500, Loss: 2.1641\n",
            "Epoch [12/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8750\n",
            "Epoch [12/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.8041\n",
            "Epoch [12/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9039\n",
            "Epoch [12/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6913\n",
            "Epoch [12/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.9604\n",
            "Epoch [12/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9068\n",
            "Epoch [12/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8944\n",
            "Epoch [12/200], Step [130/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 68.7500, Loss: 2.4246\n",
            "Epoch [12/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7975\n",
            "Epoch [12/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7433\n",
            "Overall accuracy for this epoch:  31.15047770700637\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [13/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8140\n",
            "Epoch [13/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9430\n",
            "Epoch [13/200], Step [20/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.8130\n",
            "Epoch [13/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8259\n",
            "Epoch [13/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9198\n",
            "Epoch [13/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9054\n",
            "Epoch [13/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9254\n",
            "Epoch [13/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8322\n",
            "Epoch [13/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8408\n",
            "Epoch [13/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7483\n",
            "Epoch [13/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8246\n",
            "Epoch [13/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8808\n",
            "Epoch [13/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7301\n",
            "Epoch [13/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8293\n",
            "Epoch [13/200], Step [140/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 68.7500, Loss: 2.0076\n",
            "Epoch [13/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.6266\n",
            "Overall accuracy for this epoch:  31.30971337579618\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [14/200], Step [0/157], Time (s): 2.0, Acc1: 31.2500, Acc5: 71.8750, Loss: 2.0097\n",
            "Epoch [14/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7737\n",
            "Epoch [14/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7471\n",
            "Epoch [14/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6690\n",
            "Epoch [14/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9610\n",
            "Epoch [14/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6713\n",
            "Epoch [14/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 2.1041\n",
            "Epoch [14/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8304\n",
            "Epoch [14/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.8435\n",
            "Epoch [14/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.1108\n",
            "Epoch [14/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.5921\n",
            "Epoch [14/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8531\n",
            "Epoch [14/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0167\n",
            "Epoch [14/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.9544\n",
            "Epoch [14/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.0424\n",
            "Epoch [14/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8579\n",
            "Overall accuracy for this epoch:  31.86703821656051\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [15/200], Step [0/157], Time (s): 2.0, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.0044\n",
            "Epoch [15/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6086\n",
            "Epoch [15/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9245\n",
            "Epoch [15/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.9473\n",
            "Epoch [15/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7892\n",
            "Epoch [15/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6603\n",
            "Epoch [15/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7638\n",
            "Epoch [15/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5385\n",
            "Epoch [15/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.7257\n",
            "Epoch [15/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.0045\n",
            "Epoch [15/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6439\n",
            "Epoch [15/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.6528\n",
            "Epoch [15/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 1.9331\n",
            "Epoch [15/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.7303\n",
            "Epoch [15/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8209\n",
            "Epoch [15/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0818\n",
            "Overall accuracy for this epoch:  31.588375796178344\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [16/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.6611\n",
            "Epoch [16/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8794\n",
            "Epoch [16/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7365\n",
            "Epoch [16/200], Step [30/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8599\n",
            "Epoch [16/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.8976\n",
            "Epoch [16/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.7523\n",
            "Epoch [16/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8179\n",
            "Epoch [16/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6762\n",
            "Epoch [16/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5950\n",
            "Epoch [16/200], Step [90/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 90.6250, Loss: 2.0000\n",
            "Epoch [16/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0210\n",
            "Epoch [16/200], Step [110/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.6233\n",
            "Epoch [16/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8069\n",
            "Epoch [16/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.9218\n",
            "Epoch [16/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8356\n",
            "Epoch [16/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 68.7500, Loss: 1.8260\n",
            "Overall accuracy for this epoch:  31.568471337579616\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [17/200], Step [0/157], Time (s): 2.2, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8815\n",
            "Epoch [17/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5472\n",
            "Epoch [17/200], Step [20/157], Time (s): 1.2, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.1208\n",
            "Epoch [17/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7182\n",
            "Epoch [17/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8782\n",
            "Epoch [17/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.9339\n",
            "Epoch [17/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8662\n",
            "Epoch [17/200], Step [70/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 81.2500, Loss: 1.8370\n",
            "Epoch [17/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7811\n",
            "Epoch [17/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8026\n",
            "Epoch [17/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9538\n",
            "Epoch [17/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7862\n",
            "Epoch [17/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.6898\n",
            "Epoch [17/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9590\n",
            "Epoch [17/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8714\n",
            "Epoch [17/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6627\n",
            "Overall accuracy for this epoch:  31.986464968152866\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [18/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.6791\n",
            "Epoch [18/200], Step [10/157], Time (s): 1.2, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7586\n",
            "Epoch [18/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7560\n",
            "Epoch [18/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7470\n",
            "Epoch [18/200], Step [40/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.9274\n",
            "Epoch [18/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8107\n",
            "Epoch [18/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.4930\n",
            "Epoch [18/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 2.0321\n",
            "Epoch [18/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8773\n",
            "Epoch [18/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8241\n",
            "Epoch [18/200], Step [100/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 75.0000, Loss: 2.1567\n",
            "Epoch [18/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0258\n",
            "Epoch [18/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7526\n",
            "Epoch [18/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.0708\n",
            "Epoch [18/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8752\n",
            "Epoch [18/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8513\n",
            "Overall accuracy for this epoch:  30.63296178343949\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [19/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7267\n",
            "Epoch [19/200], Step [10/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8607\n",
            "Epoch [19/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9628\n",
            "Epoch [19/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8080\n",
            "Epoch [19/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7504\n",
            "Epoch [19/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6260\n",
            "Epoch [19/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 71.8750, Loss: 1.8541\n",
            "Epoch [19/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7064\n",
            "Epoch [19/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8367\n",
            "Epoch [19/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8674\n",
            "Epoch [19/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 2.0391\n",
            "Epoch [19/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0564\n",
            "Epoch [19/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8919\n",
            "Epoch [19/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9462\n",
            "Epoch [19/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0005\n",
            "Epoch [19/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.8312\n",
            "Overall accuracy for this epoch:  32.1656050955414\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [20/200], Step [0/157], Time (s): 2.0, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8015\n",
            "Epoch [20/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9156\n",
            "Epoch [20/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8516\n",
            "Epoch [20/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7914\n",
            "Epoch [20/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.7719\n",
            "Epoch [20/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8419\n",
            "Epoch [20/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 96.8750, Loss: 1.6628\n",
            "Epoch [20/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8816\n",
            "Epoch [20/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7112\n",
            "Epoch [20/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.5912\n",
            "Epoch [20/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 2.0110\n",
            "Epoch [20/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.9795\n",
            "Epoch [20/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 2.0600\n",
            "Epoch [20/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8157\n",
            "Epoch [20/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8086\n",
            "Epoch [20/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 2.1496\n",
            "Overall accuracy for this epoch:  32.94187898089172\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [21/200], Step [0/157], Time (s): 2.0, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.1216\n",
            "Epoch [21/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7435\n",
            "Epoch [21/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7497\n",
            "Epoch [21/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6282\n",
            "Epoch [21/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7126\n",
            "Epoch [21/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7823\n",
            "Epoch [21/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8528\n",
            "Epoch [21/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 1.9048\n",
            "Epoch [21/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7421\n",
            "Epoch [21/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 100.0000, Loss: 1.6384\n",
            "Epoch [21/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8492\n",
            "Epoch [21/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.8584\n",
            "Epoch [21/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0245\n",
            "Epoch [21/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8498\n",
            "Epoch [21/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9514\n",
            "Epoch [21/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6987\n",
            "Overall accuracy for this epoch:  31.986464968152866\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [22/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 100.0000, Loss: 1.6429\n",
            "Epoch [22/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 65.6250, Loss: 1.9492\n",
            "Epoch [22/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.9118\n",
            "Epoch [22/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8349\n",
            "Epoch [22/200], Step [40/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.5326\n",
            "Epoch [22/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.9486\n",
            "Epoch [22/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0337\n",
            "Epoch [22/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6894\n",
            "Epoch [22/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8405\n",
            "Epoch [22/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7932\n",
            "Epoch [22/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9792\n",
            "Epoch [22/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 71.8750, Loss: 1.9830\n",
            "Epoch [22/200], Step [120/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.7803\n",
            "Epoch [22/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8954\n",
            "Epoch [22/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7333\n",
            "Epoch [22/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7332\n",
            "Overall accuracy for this epoch:  32.30493630573248\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [23/200], Step [0/157], Time (s): 2.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.7863\n",
            "Epoch [23/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6525\n",
            "Epoch [23/200], Step [20/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 90.6250, Loss: 1.9886\n",
            "Epoch [23/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7962\n",
            "Epoch [23/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 2.0817\n",
            "Epoch [23/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7001\n",
            "Epoch [23/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5541\n",
            "Epoch [23/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7162\n",
            "Epoch [23/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0334\n",
            "Epoch [23/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8251\n",
            "Epoch [23/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8211\n",
            "Epoch [23/200], Step [110/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.8763\n",
            "Epoch [23/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8393\n",
            "Epoch [23/200], Step [130/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9877\n",
            "Epoch [23/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8621\n",
            "Epoch [23/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6864\n",
            "Overall accuracy for this epoch:  32.245222929936304\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [24/200], Step [0/157], Time (s): 2.0, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7800\n",
            "Epoch [24/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9155\n",
            "Epoch [24/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9235\n",
            "Epoch [24/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0253\n",
            "Epoch [24/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7542\n",
            "Epoch [24/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7745\n",
            "Epoch [24/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8589\n",
            "Epoch [24/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6117\n",
            "Epoch [24/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0586\n",
            "Epoch [24/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6336\n",
            "Epoch [24/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8557\n",
            "Epoch [24/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.9064\n",
            "Epoch [24/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8058\n",
            "Epoch [24/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.0272\n",
            "Epoch [24/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.0525\n",
            "Epoch [24/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8232\n",
            "Overall accuracy for this epoch:  32.06608280254777\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [25/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.5933\n",
            "Epoch [25/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7225\n",
            "Epoch [25/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6732\n",
            "Epoch [25/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8108\n",
            "Epoch [25/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7418\n",
            "Epoch [25/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6034\n",
            "Epoch [25/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.0684\n",
            "Epoch [25/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 75.0000, Loss: 1.9089\n",
            "Epoch [25/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8197\n",
            "Epoch [25/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9806\n",
            "Epoch [25/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6475\n",
            "Epoch [25/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6752\n",
            "Epoch [25/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6570\n",
            "Epoch [25/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 2.1181\n",
            "Epoch [25/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7560\n",
            "Epoch [25/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7734\n",
            "Overall accuracy for this epoch:  32.12579617834395\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [26/200], Step [0/157], Time (s): 2.0, Acc1: 28.1250, Acc5: 68.7500, Loss: 1.9946\n",
            "Epoch [26/200], Step [10/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 78.1250, Loss: 2.0475\n",
            "Epoch [26/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7882\n",
            "Epoch [26/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9882\n",
            "Epoch [26/200], Step [40/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 2.0203\n",
            "Epoch [26/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7565\n",
            "Epoch [26/200], Step [60/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.6691\n",
            "Epoch [26/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 2.0192\n",
            "Epoch [26/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8769\n",
            "Epoch [26/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8876\n",
            "Epoch [26/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7147\n",
            "Epoch [26/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8009\n",
            "Epoch [26/200], Step [120/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 78.1250, Loss: 2.0209\n",
            "Epoch [26/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8524\n",
            "Epoch [26/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7950\n",
            "Epoch [26/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9891\n",
            "Overall accuracy for this epoch:  32.5437898089172\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [27/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9224\n",
            "Epoch [27/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8635\n",
            "Epoch [27/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7924\n",
            "Epoch [27/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9629\n",
            "Epoch [27/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9876\n",
            "Epoch [27/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 96.8750, Loss: 1.6454\n",
            "Epoch [27/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.1706\n",
            "Epoch [27/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.7803\n",
            "Epoch [27/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6924\n",
            "Epoch [27/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8403\n",
            "Epoch [27/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6988\n",
            "Epoch [27/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.9100\n",
            "Epoch [27/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8547\n",
            "Epoch [27/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9094\n",
            "Epoch [27/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.6985\n",
            "Epoch [27/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9116\n",
            "Overall accuracy for this epoch:  33.0015923566879\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [28/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7819\n",
            "Epoch [28/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 68.7500, Loss: 2.0438\n",
            "Epoch [28/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7598\n",
            "Epoch [28/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 68.7500, Loss: 1.9140\n",
            "Epoch [28/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.9994\n",
            "Epoch [28/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8680\n",
            "Epoch [28/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7865\n",
            "Epoch [28/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7430\n",
            "Epoch [28/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.2006\n",
            "Epoch [28/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8484\n",
            "Epoch [28/200], Step [100/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5944\n",
            "Epoch [28/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8388\n",
            "Epoch [28/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8669\n",
            "Epoch [28/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9979\n",
            "Epoch [28/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7974\n",
            "Epoch [28/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7821\n",
            "Overall accuracy for this epoch:  33.18073248407644\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [29/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7427\n",
            "Epoch [29/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 71.8750, Loss: 2.0424\n",
            "Epoch [29/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.9009\n",
            "Epoch [29/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9234\n",
            "Epoch [29/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 65.6250, Loss: 1.9777\n",
            "Epoch [29/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7388\n",
            "Epoch [29/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9116\n",
            "Epoch [29/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7357\n",
            "Epoch [29/200], Step [80/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.4933\n",
            "Epoch [29/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.8081\n",
            "Epoch [29/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6530\n",
            "Epoch [29/200], Step [110/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.6852\n",
            "Epoch [29/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.9756\n",
            "Epoch [29/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7696\n",
            "Epoch [29/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8721\n",
            "Epoch [29/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0806\n",
            "Overall accuracy for this epoch:  31.926751592356688\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [30/200], Step [0/157], Time (s): 2.0, Acc1: 18.7500, Acc5: 84.3750, Loss: 2.0436\n",
            "Epoch [30/200], Step [10/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.8537\n",
            "Epoch [30/200], Step [20/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.8287\n",
            "Epoch [30/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8258\n",
            "Epoch [30/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.6545\n",
            "Epoch [30/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6327\n",
            "Epoch [30/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8827\n",
            "Epoch [30/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0811\n",
            "Epoch [30/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.8787\n",
            "Epoch [30/200], Step [90/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5836\n",
            "Epoch [30/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6747\n",
            "Epoch [30/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8091\n",
            "Epoch [30/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7165\n",
            "Epoch [30/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8217\n",
            "Epoch [30/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8743\n",
            "Epoch [30/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8502\n",
            "Overall accuracy for this epoch:  32.36464968152866\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [31/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.6979\n",
            "Epoch [31/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7902\n",
            "Epoch [31/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8591\n",
            "Epoch [31/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8249\n",
            "Epoch [31/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8243\n",
            "Epoch [31/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8918\n",
            "Epoch [31/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5258\n",
            "Epoch [31/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9385\n",
            "Epoch [31/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8554\n",
            "Epoch [31/200], Step [90/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.6743\n",
            "Epoch [31/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8024\n",
            "Epoch [31/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9389\n",
            "Epoch [31/200], Step [120/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.4608\n",
            "Epoch [31/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 1.7631\n",
            "Epoch [31/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.1859\n",
            "Epoch [31/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.1064\n",
            "Overall accuracy for this epoch:  32.50398089171974\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [32/200], Step [0/157], Time (s): 2.0, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6921\n",
            "Epoch [32/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6190\n",
            "Epoch [32/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8141\n",
            "Epoch [32/200], Step [30/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 84.3750, Loss: 1.6686\n",
            "Epoch [32/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8934\n",
            "Epoch [32/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.9458\n",
            "Epoch [32/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8263\n",
            "Epoch [32/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8095\n",
            "Epoch [32/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8231\n",
            "Epoch [32/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8456\n",
            "Epoch [32/200], Step [100/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8892\n",
            "Epoch [32/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6832\n",
            "Epoch [32/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7651\n",
            "Epoch [32/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8408\n",
            "Epoch [32/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.6529\n",
            "Epoch [32/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8696\n",
            "Overall accuracy for this epoch:  32.44426751592356\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [33/200], Step [0/157], Time (s): 2.0, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9169\n",
            "Epoch [33/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9413\n",
            "Epoch [33/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 2.0190\n",
            "Epoch [33/200], Step [30/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.9895\n",
            "Epoch [33/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9306\n",
            "Epoch [33/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6534\n",
            "Epoch [33/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9807\n",
            "Epoch [33/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8341\n",
            "Epoch [33/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8289\n",
            "Epoch [33/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7492\n",
            "Epoch [33/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7456\n",
            "Epoch [33/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.9882\n",
            "Epoch [33/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6838\n",
            "Epoch [33/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7338\n",
            "Epoch [33/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 71.8750, Loss: 2.0305\n",
            "Epoch [33/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8830\n",
            "Overall accuracy for this epoch:  32.76273885350319\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [34/200], Step [0/157], Time (s): 2.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6450\n",
            "Epoch [34/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9990\n",
            "Epoch [34/200], Step [20/157], Time (s): 1.2, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.2316\n",
            "Epoch [34/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7168\n",
            "Epoch [34/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9535\n",
            "Epoch [34/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7089\n",
            "Epoch [34/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9755\n",
            "Epoch [34/200], Step [70/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.7901\n",
            "Epoch [34/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7053\n",
            "Epoch [34/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8585\n",
            "Epoch [34/200], Step [100/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.9544\n",
            "Epoch [34/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 96.8750, Loss: 1.7663\n",
            "Epoch [34/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 1.9491\n",
            "Epoch [34/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9416\n",
            "Epoch [34/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7440\n",
            "Epoch [34/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7375\n",
            "Overall accuracy for this epoch:  33.04140127388535\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [35/200], Step [0/157], Time (s): 2.0, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0097\n",
            "Epoch [35/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7498\n",
            "Epoch [35/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 96.8750, Loss: 1.6830\n",
            "Epoch [35/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.6679\n",
            "Epoch [35/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0471\n",
            "Epoch [35/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9233\n",
            "Epoch [35/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8487\n",
            "Epoch [35/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8324\n",
            "Epoch [35/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6791\n",
            "Epoch [35/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.9959\n",
            "Epoch [35/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9779\n",
            "Epoch [35/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8327\n",
            "Epoch [35/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9479\n",
            "Epoch [35/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8242\n",
            "Epoch [35/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 68.7500, Loss: 2.2517\n",
            "Epoch [35/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7885\n",
            "Overall accuracy for this epoch:  32.5437898089172\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [36/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7738\n",
            "Epoch [36/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.9319\n",
            "Epoch [36/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9048\n",
            "Epoch [36/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9020\n",
            "Epoch [36/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7449\n",
            "Epoch [36/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9226\n",
            "Epoch [36/200], Step [60/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.5581\n",
            "Epoch [36/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 75.0000, Loss: 1.8272\n",
            "Epoch [36/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8212\n",
            "Epoch [36/200], Step [90/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.5323\n",
            "Epoch [36/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7439\n",
            "Epoch [36/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7157\n",
            "Epoch [36/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5432\n",
            "Epoch [36/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8438\n",
            "Epoch [36/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9056\n",
            "Epoch [36/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.8624\n",
            "Overall accuracy for this epoch:  32.12579617834395\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [37/200], Step [0/157], Time (s): 2.0, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9191\n",
            "Epoch [37/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8089\n",
            "Epoch [37/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8825\n",
            "Epoch [37/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8049\n",
            "Epoch [37/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.0431\n",
            "Epoch [37/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7766\n",
            "Epoch [37/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9312\n",
            "Epoch [37/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7684\n",
            "Epoch [37/200], Step [80/157], Time (s): 1.1, Acc1: 9.3750, Acc5: 90.6250, Loss: 1.9733\n",
            "Epoch [37/200], Step [90/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6624\n",
            "Epoch [37/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9473\n",
            "Epoch [37/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 65.6250, Loss: 2.0261\n",
            "Epoch [37/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8690\n",
            "Epoch [37/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9455\n",
            "Epoch [37/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.5962\n",
            "Epoch [37/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7249\n",
            "Overall accuracy for this epoch:  32.603503184713375\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [38/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6401\n",
            "Epoch [38/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 2.0511\n",
            "Epoch [38/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.9095\n",
            "Epoch [38/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7768\n",
            "Epoch [38/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 68.7500, Loss: 2.0173\n",
            "Epoch [38/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8319\n",
            "Epoch [38/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9711\n",
            "Epoch [38/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9230\n",
            "Epoch [38/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0876\n",
            "Epoch [38/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7495\n",
            "Epoch [38/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9826\n",
            "Epoch [38/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8146\n",
            "Epoch [38/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7648\n",
            "Epoch [38/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.6464\n",
            "Epoch [38/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8933\n",
            "Epoch [38/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0548\n",
            "Overall accuracy for this epoch:  33.24044585987261\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [39/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7478\n",
            "Epoch [39/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.9341\n",
            "Epoch [39/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.7248\n",
            "Epoch [39/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9317\n",
            "Epoch [39/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8199\n",
            "Epoch [39/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9352\n",
            "Epoch [39/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.8472\n",
            "Epoch [39/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.9329\n",
            "Epoch [39/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0874\n",
            "Epoch [39/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6349\n",
            "Epoch [39/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7178\n",
            "Epoch [39/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.9617\n",
            "Epoch [39/200], Step [120/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 84.3750, Loss: 1.8929\n",
            "Epoch [39/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7918\n",
            "Epoch [39/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 78.1250, Loss: 1.8487\n",
            "Epoch [39/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6602\n",
            "Overall accuracy for this epoch:  32.86226114649681\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [40/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8910\n",
            "Epoch [40/200], Step [10/157], Time (s): 1.2, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9542\n",
            "Epoch [40/200], Step [20/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.5730\n",
            "Epoch [40/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8387\n",
            "Epoch [40/200], Step [40/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.8552\n",
            "Epoch [40/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8364\n",
            "Epoch [40/200], Step [60/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.9278\n",
            "Epoch [40/200], Step [70/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5841\n",
            "Epoch [40/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8965\n",
            "Epoch [40/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.8144\n",
            "Epoch [40/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7918\n",
            "Epoch [40/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9135\n",
            "Epoch [40/200], Step [120/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0132\n",
            "Epoch [40/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8701\n",
            "Epoch [40/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8489\n",
            "Epoch [40/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.8008\n",
            "Overall accuracy for this epoch:  32.92197452229299\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [41/200], Step [0/157], Time (s): 2.0, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5679\n",
            "Epoch [41/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8039\n",
            "Epoch [41/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8672\n",
            "Epoch [41/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7986\n",
            "Epoch [41/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7184\n",
            "Epoch [41/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8094\n",
            "Epoch [41/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6797\n",
            "Epoch [41/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7877\n",
            "Epoch [41/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7198\n",
            "Epoch [41/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7693\n",
            "Epoch [41/200], Step [100/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8105\n",
            "Epoch [41/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8040\n",
            "Epoch [41/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9579\n",
            "Epoch [41/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.8570\n",
            "Epoch [41/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7804\n",
            "Epoch [41/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7176\n",
            "Overall accuracy for this epoch:  32.94187898089172\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [42/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7934\n",
            "Epoch [42/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6299\n",
            "Epoch [42/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9964\n",
            "Epoch [42/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9759\n",
            "Epoch [42/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8584\n",
            "Epoch [42/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8448\n",
            "Epoch [42/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8909\n",
            "Epoch [42/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8975\n",
            "Epoch [42/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8120\n",
            "Epoch [42/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.7288\n",
            "Epoch [42/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6864\n",
            "Epoch [42/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0057\n",
            "Epoch [42/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6486\n",
            "Epoch [42/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6501\n",
            "Epoch [42/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7662\n",
            "Epoch [42/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 2.0470\n",
            "Overall accuracy for this epoch:  32.961783439490446\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [43/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7265\n",
            "Epoch [43/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9698\n",
            "Epoch [43/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.5927\n",
            "Epoch [43/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9999\n",
            "Epoch [43/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8096\n",
            "Epoch [43/200], Step [50/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.9049\n",
            "Epoch [43/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.5976\n",
            "Epoch [43/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7752\n",
            "Epoch [43/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8181\n",
            "Epoch [43/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.6564\n",
            "Epoch [43/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8559\n",
            "Epoch [43/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7623\n",
            "Epoch [43/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8599\n",
            "Epoch [43/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7616\n",
            "Epoch [43/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.8676\n",
            "Epoch [43/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8791\n",
            "Overall accuracy for this epoch:  32.50398089171974\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [44/200], Step [0/157], Time (s): 2.0, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8202\n",
            "Epoch [44/200], Step [10/157], Time (s): 1.2, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7741\n",
            "Epoch [44/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7951\n",
            "Epoch [44/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8698\n",
            "Epoch [44/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7690\n",
            "Epoch [44/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.0511\n",
            "Epoch [44/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.7854\n",
            "Epoch [44/200], Step [70/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 96.8750, Loss: 1.4624\n",
            "Epoch [44/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7081\n",
            "Epoch [44/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8410\n",
            "Epoch [44/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9484\n",
            "Epoch [44/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7045\n",
            "Epoch [44/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9737\n",
            "Epoch [44/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9883\n",
            "Epoch [44/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8606\n",
            "Epoch [44/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 2.1270\n",
            "Overall accuracy for this epoch:  33.89729299363057\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [45/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6276\n",
            "Epoch [45/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8245\n",
            "Epoch [45/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8869\n",
            "Epoch [45/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.4956\n",
            "Epoch [45/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.9565\n",
            "Epoch [45/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8352\n",
            "Epoch [45/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5290\n",
            "Epoch [45/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.8628\n",
            "Epoch [45/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9300\n",
            "Epoch [45/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8286\n",
            "Epoch [45/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.7938\n",
            "Epoch [45/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 2.0573\n",
            "Epoch [45/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9779\n",
            "Epoch [45/200], Step [130/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.6095\n",
            "Epoch [45/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8588\n",
            "Epoch [45/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8960\n",
            "Overall accuracy for this epoch:  32.802547770700635\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [46/200], Step [0/157], Time (s): 2.2, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.1514\n",
            "Epoch [46/200], Step [10/157], Time (s): 1.2, Acc1: 18.7500, Acc5: 68.7500, Loss: 2.1943\n",
            "Epoch [46/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7743\n",
            "Epoch [46/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8561\n",
            "Epoch [46/200], Step [40/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.8514\n",
            "Epoch [46/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6721\n",
            "Epoch [46/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7017\n",
            "Epoch [46/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8469\n",
            "Epoch [46/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7969\n",
            "Epoch [46/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9130\n",
            "Epoch [46/200], Step [100/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.6585\n",
            "Epoch [46/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 2.2399\n",
            "Epoch [46/200], Step [120/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8708\n",
            "Epoch [46/200], Step [130/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 78.1250, Loss: 2.0768\n",
            "Epoch [46/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7818\n",
            "Epoch [46/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7681\n",
            "Overall accuracy for this epoch:  33.359872611464965\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [47/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8162\n",
            "Epoch [47/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7736\n",
            "Epoch [47/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6804\n",
            "Epoch [47/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8332\n",
            "Epoch [47/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7793\n",
            "Epoch [47/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8648\n",
            "Epoch [47/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7640\n",
            "Epoch [47/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.9522\n",
            "Epoch [47/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6487\n",
            "Epoch [47/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7266\n",
            "Epoch [47/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6852\n",
            "Epoch [47/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 2.0781\n",
            "Epoch [47/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7889\n",
            "Epoch [47/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6765\n",
            "Epoch [47/200], Step [140/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.7166\n",
            "Epoch [47/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8218\n",
            "Overall accuracy for this epoch:  32.42436305732484\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [48/200], Step [0/157], Time (s): 2.0, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8932\n",
            "Epoch [48/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7899\n",
            "Epoch [48/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6101\n",
            "Epoch [48/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 96.8750, Loss: 1.6460\n",
            "Epoch [48/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 100.0000, Loss: 1.5829\n",
            "Epoch [48/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.8522\n",
            "Epoch [48/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 78.1250, Loss: 1.8391\n",
            "Epoch [48/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9503\n",
            "Epoch [48/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9192\n",
            "Epoch [48/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0769\n",
            "Epoch [48/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8346\n",
            "Epoch [48/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7620\n",
            "Epoch [48/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 1.9574\n",
            "Epoch [48/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7743\n",
            "Epoch [48/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9600\n",
            "Epoch [48/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7708\n",
            "Overall accuracy for this epoch:  33.47929936305732\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [49/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.7959\n",
            "Epoch [49/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6284\n",
            "Epoch [49/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7980\n",
            "Epoch [49/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6958\n",
            "Epoch [49/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6648\n",
            "Epoch [49/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8272\n",
            "Epoch [49/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8441\n",
            "Epoch [49/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7298\n",
            "Epoch [49/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.5897\n",
            "Epoch [49/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6667\n",
            "Epoch [49/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6046\n",
            "Epoch [49/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.7143\n",
            "Epoch [49/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8769\n",
            "Epoch [49/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7258\n",
            "Epoch [49/200], Step [140/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 93.7500, Loss: 1.4088\n",
            "Epoch [49/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7487\n",
            "Overall accuracy for this epoch:  33.0015923566879\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [50/200], Step [0/157], Time (s): 2.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8221\n",
            "Epoch [50/200], Step [10/157], Time (s): 1.2, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8797\n",
            "Epoch [50/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8823\n",
            "Epoch [50/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9473\n",
            "Epoch [50/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9318\n",
            "Epoch [50/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7003\n",
            "Epoch [50/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7021\n",
            "Epoch [50/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7973\n",
            "Epoch [50/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 2.1025\n",
            "Epoch [50/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7897\n",
            "Epoch [50/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8443\n",
            "Epoch [50/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7355\n",
            "Epoch [50/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9526\n",
            "Epoch [50/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9059\n",
            "Epoch [50/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.7940\n",
            "Epoch [50/200], Step [150/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.5870\n",
            "Overall accuracy for this epoch:  34.27547770700637\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [51/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7555\n",
            "Epoch [51/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 2.0510\n",
            "Epoch [51/200], Step [20/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.6954\n",
            "Epoch [51/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7525\n",
            "Epoch [51/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7752\n",
            "Epoch [51/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6254\n",
            "Epoch [51/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7675\n",
            "Epoch [51/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6860\n",
            "Epoch [51/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7473\n",
            "Epoch [51/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.9460\n",
            "Epoch [51/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6009\n",
            "Epoch [51/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8922\n",
            "Epoch [51/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8478\n",
            "Epoch [51/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6365\n",
            "Epoch [51/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5094\n",
            "Epoch [51/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9761\n",
            "Overall accuracy for this epoch:  33.55891719745223\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [52/200], Step [0/157], Time (s): 2.0, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.5641\n",
            "Epoch [52/200], Step [10/157], Time (s): 1.2, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8275\n",
            "Epoch [52/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7720\n",
            "Epoch [52/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7756\n",
            "Epoch [52/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6271\n",
            "Epoch [52/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9800\n",
            "Epoch [52/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8131\n",
            "Epoch [52/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8276\n",
            "Epoch [52/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.8340\n",
            "Epoch [52/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9237\n",
            "Epoch [52/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8305\n",
            "Epoch [52/200], Step [110/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.1010\n",
            "Epoch [52/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7978\n",
            "Epoch [52/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6636\n",
            "Epoch [52/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8123\n",
            "Epoch [52/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0990\n",
            "Overall accuracy for this epoch:  33.18073248407644\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [53/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7775\n",
            "Epoch [53/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8268\n",
            "Epoch [53/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9523\n",
            "Epoch [53/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.9673\n",
            "Epoch [53/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7308\n",
            "Epoch [53/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8003\n",
            "Epoch [53/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.7837\n",
            "Epoch [53/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7482\n",
            "Epoch [53/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7925\n",
            "Epoch [53/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6402\n",
            "Epoch [53/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5793\n",
            "Epoch [53/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8863\n",
            "Epoch [53/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7780\n",
            "Epoch [53/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9484\n",
            "Epoch [53/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9088\n",
            "Epoch [53/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.7076\n",
            "Overall accuracy for this epoch:  32.822452229299365\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [54/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6920\n",
            "Epoch [54/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7692\n",
            "Epoch [54/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8459\n",
            "Epoch [54/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7359\n",
            "Epoch [54/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7958\n",
            "Epoch [54/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7606\n",
            "Epoch [54/200], Step [60/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 75.0000, Loss: 1.9861\n",
            "Epoch [54/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8933\n",
            "Epoch [54/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9765\n",
            "Epoch [54/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.5477\n",
            "Epoch [54/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.0346\n",
            "Epoch [54/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6266\n",
            "Epoch [54/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6685\n",
            "Epoch [54/200], Step [130/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5187\n",
            "Epoch [54/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7263\n",
            "Epoch [54/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0109\n",
            "Overall accuracy for this epoch:  33.638535031847134\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [55/200], Step [0/157], Time (s): 2.0, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.7469\n",
            "Epoch [55/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8128\n",
            "Epoch [55/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8022\n",
            "Epoch [55/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7787\n",
            "Epoch [55/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8725\n",
            "Epoch [55/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6346\n",
            "Epoch [55/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.9052\n",
            "Epoch [55/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9005\n",
            "Epoch [55/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6005\n",
            "Epoch [55/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6937\n",
            "Epoch [55/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.9937\n",
            "Epoch [55/200], Step [110/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.5864\n",
            "Epoch [55/200], Step [120/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 93.7500, Loss: 1.9834\n",
            "Epoch [55/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8917\n",
            "Epoch [55/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6630\n",
            "Epoch [55/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.9088\n",
            "Overall accuracy for this epoch:  33.14092356687898\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [56/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.6822\n",
            "Epoch [56/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8470\n",
            "Epoch [56/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.5701\n",
            "Epoch [56/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.7205\n",
            "Epoch [56/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8571\n",
            "Epoch [56/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5855\n",
            "Epoch [56/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8259\n",
            "Epoch [56/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7603\n",
            "Epoch [56/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.9013\n",
            "Epoch [56/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6821\n",
            "Epoch [56/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7344\n",
            "Epoch [56/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9522\n",
            "Epoch [56/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 100.0000, Loss: 1.5987\n",
            "Epoch [56/200], Step [130/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.6330\n",
            "Epoch [56/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7465\n",
            "Epoch [56/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7037\n",
            "Overall accuracy for this epoch:  34.01671974522293\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [57/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7969\n",
            "Epoch [57/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8596\n",
            "Epoch [57/200], Step [20/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0281\n",
            "Epoch [57/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.4940\n",
            "Epoch [57/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7759\n",
            "Epoch [57/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.5994\n",
            "Epoch [57/200], Step [60/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5545\n",
            "Epoch [57/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.5817\n",
            "Epoch [57/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7644\n",
            "Epoch [57/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7966\n",
            "Epoch [57/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7524\n",
            "Epoch [57/200], Step [110/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5524\n",
            "Epoch [57/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.5734\n",
            "Epoch [57/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7429\n",
            "Epoch [57/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6976\n",
            "Epoch [57/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6363\n",
            "Overall accuracy for this epoch:  33.4593949044586\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [58/200], Step [0/157], Time (s): 2.0, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7823\n",
            "Epoch [58/200], Step [10/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6791\n",
            "Epoch [58/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.7838\n",
            "Epoch [58/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 2.0740\n",
            "Epoch [58/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7709\n",
            "Epoch [58/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 2.1229\n",
            "Epoch [58/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7324\n",
            "Epoch [58/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 2.0754\n",
            "Epoch [58/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7998\n",
            "Epoch [58/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 2.1458\n",
            "Epoch [58/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5796\n",
            "Epoch [58/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6140\n",
            "Epoch [58/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7076\n",
            "Epoch [58/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6856\n",
            "Epoch [58/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6923\n",
            "Epoch [58/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 1.9878\n",
            "Overall accuracy for this epoch:  33.220541401273884\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [59/200], Step [0/157], Time (s): 2.0, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8209\n",
            "Epoch [59/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7560\n",
            "Epoch [59/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.7319\n",
            "Epoch [59/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9285\n",
            "Epoch [59/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8235\n",
            "Epoch [59/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.8232\n",
            "Epoch [59/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8425\n",
            "Epoch [59/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.1011\n",
            "Epoch [59/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8633\n",
            "Epoch [59/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8539\n",
            "Epoch [59/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8640\n",
            "Epoch [59/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8382\n",
            "Epoch [59/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6564\n",
            "Epoch [59/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8006\n",
            "Epoch [59/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6525\n",
            "Epoch [59/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9165\n",
            "Overall accuracy for this epoch:  34.21576433121019\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [60/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 71.8750, Loss: 1.8796\n",
            "Epoch [60/200], Step [10/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.9498\n",
            "Epoch [60/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.6002\n",
            "Epoch [60/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7319\n",
            "Epoch [60/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.6602\n",
            "Epoch [60/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7106\n",
            "Epoch [60/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7970\n",
            "Epoch [60/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8653\n",
            "Epoch [60/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.6830\n",
            "Epoch [60/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.6776\n",
            "Epoch [60/200], Step [100/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.7521\n",
            "Epoch [60/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7041\n",
            "Epoch [60/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7098\n",
            "Epoch [60/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7505\n",
            "Epoch [60/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 2.0050\n",
            "Epoch [60/200], Step [150/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.5868\n",
            "Overall accuracy for this epoch:  33.18073248407644\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [61/200], Step [0/157], Time (s): 2.0, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5793\n",
            "Epoch [61/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8034\n",
            "Epoch [61/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7270\n",
            "Epoch [61/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7856\n",
            "Epoch [61/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7563\n",
            "Epoch [61/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 68.7500, Loss: 2.2318\n",
            "Epoch [61/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8837\n",
            "Epoch [61/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6175\n",
            "Epoch [61/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7153\n",
            "Epoch [61/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9853\n",
            "Epoch [61/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8287\n",
            "Epoch [61/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6622\n",
            "Epoch [61/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9580\n",
            "Epoch [61/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9274\n",
            "Epoch [61/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9363\n",
            "Epoch [61/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0114\n",
            "Overall accuracy for this epoch:  33.14092356687898\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [62/200], Step [0/157], Time (s): 2.0, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8374\n",
            "Epoch [62/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.8793\n",
            "Epoch [62/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6452\n",
            "Epoch [62/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8570\n",
            "Epoch [62/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0198\n",
            "Epoch [62/200], Step [50/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.6109\n",
            "Epoch [62/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9651\n",
            "Epoch [62/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6322\n",
            "Epoch [62/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7598\n",
            "Epoch [62/200], Step [90/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6711\n",
            "Epoch [62/200], Step [100/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 78.1250, Loss: 2.2498\n",
            "Epoch [62/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6974\n",
            "Epoch [62/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6374\n",
            "Epoch [62/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.9562\n",
            "Epoch [62/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7519\n",
            "Epoch [62/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.6427\n",
            "Overall accuracy for this epoch:  34.65366242038217\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [63/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7408\n",
            "Epoch [63/200], Step [10/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 90.6250, Loss: 1.9036\n",
            "Epoch [63/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 2.0304\n",
            "Epoch [63/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8282\n",
            "Epoch [63/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6918\n",
            "Epoch [63/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7984\n",
            "Epoch [63/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 71.8750, Loss: 2.0677\n",
            "Epoch [63/200], Step [70/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8642\n",
            "Epoch [63/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8023\n",
            "Epoch [63/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7500\n",
            "Epoch [63/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9575\n",
            "Epoch [63/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9144\n",
            "Epoch [63/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.8809\n",
            "Epoch [63/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 100.0000, Loss: 1.5534\n",
            "Epoch [63/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9491\n",
            "Epoch [63/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7767\n",
            "Overall accuracy for this epoch:  33.55891719745223\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [64/200], Step [0/157], Time (s): 2.0, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5267\n",
            "Epoch [64/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7329\n",
            "Epoch [64/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9313\n",
            "Epoch [64/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9233\n",
            "Epoch [64/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8168\n",
            "Epoch [64/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7354\n",
            "Epoch [64/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6629\n",
            "Epoch [64/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 2.0057\n",
            "Epoch [64/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8722\n",
            "Epoch [64/200], Step [90/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 81.2500, Loss: 1.9221\n",
            "Epoch [64/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.7265\n",
            "Epoch [64/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8417\n",
            "Epoch [64/200], Step [120/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 93.7500, Loss: 1.6291\n",
            "Epoch [64/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6491\n",
            "Epoch [64/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7851\n",
            "Epoch [64/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 2.0050\n",
            "Overall accuracy for this epoch:  33.5390127388535\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [65/200], Step [0/157], Time (s): 2.0, Acc1: 25.0000, Acc5: 96.8750, Loss: 1.7852\n",
            "Epoch [65/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0086\n",
            "Epoch [65/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7012\n",
            "Epoch [65/200], Step [30/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9527\n",
            "Epoch [65/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8725\n",
            "Epoch [65/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9583\n",
            "Epoch [65/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.7236\n",
            "Epoch [65/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8545\n",
            "Epoch [65/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7271\n",
            "Epoch [65/200], Step [90/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9401\n",
            "Epoch [65/200], Step [100/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 75.0000, Loss: 2.2461\n",
            "Epoch [65/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6370\n",
            "Epoch [65/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.8500\n",
            "Epoch [65/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7479\n",
            "Epoch [65/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8779\n",
            "Epoch [65/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8956\n",
            "Overall accuracy for this epoch:  33.67834394904459\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [66/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7576\n",
            "Epoch [66/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7549\n",
            "Epoch [66/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7637\n",
            "Epoch [66/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8402\n",
            "Epoch [66/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.9719\n",
            "Epoch [66/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 2.0282\n",
            "Epoch [66/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9801\n",
            "Epoch [66/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8532\n",
            "Epoch [66/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8134\n",
            "Epoch [66/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8935\n",
            "Epoch [66/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7447\n",
            "Epoch [66/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0842\n",
            "Epoch [66/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7122\n",
            "Epoch [66/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5438\n",
            "Epoch [66/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8671\n",
            "Epoch [66/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7341\n",
            "Overall accuracy for this epoch:  33.638535031847134\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [67/200], Step [0/157], Time (s): 2.0, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7045\n",
            "Epoch [67/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.5591\n",
            "Epoch [67/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.6528\n",
            "Epoch [67/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9291\n",
            "Epoch [67/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8678\n",
            "Epoch [67/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.7332\n",
            "Epoch [67/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6492\n",
            "Epoch [67/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8772\n",
            "Epoch [67/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7452\n",
            "Epoch [67/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9842\n",
            "Epoch [67/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8078\n",
            "Epoch [67/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8342\n",
            "Epoch [67/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.9902\n",
            "Epoch [67/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8858\n",
            "Epoch [67/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9616\n",
            "Epoch [67/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8001\n",
            "Overall accuracy for this epoch:  33.24044585987261\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [68/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7609\n",
            "Epoch [68/200], Step [10/157], Time (s): 1.2, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.5782\n",
            "Epoch [68/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8081\n",
            "Epoch [68/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.8344\n",
            "Epoch [68/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7418\n",
            "Epoch [68/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7674\n",
            "Epoch [68/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.8989\n",
            "Epoch [68/200], Step [70/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.5797\n",
            "Epoch [68/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7073\n",
            "Epoch [68/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9015\n",
            "Epoch [68/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7824\n",
            "Epoch [68/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 68.7500, Loss: 2.0911\n",
            "Epoch [68/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9978\n",
            "Epoch [68/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7889\n",
            "Epoch [68/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8787\n",
            "Epoch [68/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8868\n",
            "Overall accuracy for this epoch:  34.056528662420384\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [69/200], Step [0/157], Time (s): 2.0, Acc1: 62.5000, Acc5: 90.6250, Loss: 1.6171\n",
            "Epoch [69/200], Step [10/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 93.7500, Loss: 1.4906\n",
            "Epoch [69/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8052\n",
            "Epoch [69/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7444\n",
            "Epoch [69/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9228\n",
            "Epoch [69/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9030\n",
            "Epoch [69/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7860\n",
            "Epoch [69/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7183\n",
            "Epoch [69/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7898\n",
            "Epoch [69/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8643\n",
            "Epoch [69/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 2.1177\n",
            "Epoch [69/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0181\n",
            "Epoch [69/200], Step [120/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5157\n",
            "Epoch [69/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.6426\n",
            "Epoch [69/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7284\n",
            "Epoch [69/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.8355\n",
            "Overall accuracy for this epoch:  34.27547770700637\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [70/200], Step [0/157], Time (s): 2.0, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.7051\n",
            "Epoch [70/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7386\n",
            "Epoch [70/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5431\n",
            "Epoch [70/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7386\n",
            "Epoch [70/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9376\n",
            "Epoch [70/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6554\n",
            "Epoch [70/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0788\n",
            "Epoch [70/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7836\n",
            "Epoch [70/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6872\n",
            "Epoch [70/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8640\n",
            "Epoch [70/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 2.0181\n",
            "Epoch [70/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8784\n",
            "Epoch [70/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 2.0036\n",
            "Epoch [70/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8420\n",
            "Epoch [70/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.6369\n",
            "Epoch [70/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 71.8750, Loss: 1.8761\n",
            "Overall accuracy for this epoch:  34.375\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [71/200], Step [0/157], Time (s): 2.0, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8321\n",
            "Epoch [71/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8026\n",
            "Epoch [71/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6450\n",
            "Epoch [71/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8456\n",
            "Epoch [71/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8060\n",
            "Epoch [71/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7147\n",
            "Epoch [71/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.1547\n",
            "Epoch [71/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 2.0847\n",
            "Epoch [71/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8133\n",
            "Epoch [71/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6456\n",
            "Epoch [71/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8974\n",
            "Epoch [71/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7286\n",
            "Epoch [71/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8114\n",
            "Epoch [71/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8203\n",
            "Epoch [71/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6419\n",
            "Epoch [71/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6984\n",
            "Overall accuracy for this epoch:  33.93710191082803\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [72/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8337\n",
            "Epoch [72/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 100.0000, Loss: 1.5806\n",
            "Epoch [72/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9992\n",
            "Epoch [72/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 2.0586\n",
            "Epoch [72/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8346\n",
            "Epoch [72/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8725\n",
            "Epoch [72/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.6896\n",
            "Epoch [72/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9016\n",
            "Epoch [72/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7785\n",
            "Epoch [72/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7884\n",
            "Epoch [72/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6746\n",
            "Epoch [72/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6729\n",
            "Epoch [72/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6140\n",
            "Epoch [72/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.1425\n",
            "Epoch [72/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7038\n",
            "Epoch [72/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7837\n",
            "Overall accuracy for this epoch:  34.11624203821656\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [73/200], Step [0/157], Time (s): 2.0, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6639\n",
            "Epoch [73/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 2.0290\n",
            "Epoch [73/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.7847\n",
            "Epoch [73/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7709\n",
            "Epoch [73/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8168\n",
            "Epoch [73/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8200\n",
            "Epoch [73/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8066\n",
            "Epoch [73/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6905\n",
            "Epoch [73/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7477\n",
            "Epoch [73/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8202\n",
            "Epoch [73/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.8733\n",
            "Epoch [73/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.9356\n",
            "Epoch [73/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7557\n",
            "Epoch [73/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8731\n",
            "Epoch [73/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 2.1357\n",
            "Epoch [73/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6953\n",
            "Overall accuracy for this epoch:  33.32006369426752\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [74/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8219\n",
            "Epoch [74/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.8546\n",
            "Epoch [74/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8205\n",
            "Epoch [74/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.5859\n",
            "Epoch [74/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7490\n",
            "Epoch [74/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.6297\n",
            "Epoch [74/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9666\n",
            "Epoch [74/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6526\n",
            "Epoch [74/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 65.6250, Loss: 2.2624\n",
            "Epoch [74/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6379\n",
            "Epoch [74/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9242\n",
            "Epoch [74/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.0026\n",
            "Epoch [74/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6705\n",
            "Epoch [74/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6440\n",
            "Epoch [74/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6295\n",
            "Epoch [74/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9665\n",
            "Overall accuracy for this epoch:  34.136146496815286\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [75/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.6725\n",
            "Epoch [75/200], Step [10/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 96.8750, Loss: 1.5006\n",
            "Epoch [75/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 71.8750, Loss: 1.9479\n",
            "Epoch [75/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7783\n",
            "Epoch [75/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7793\n",
            "Epoch [75/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7779\n",
            "Epoch [75/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8049\n",
            "Epoch [75/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9377\n",
            "Epoch [75/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.7937\n",
            "Epoch [75/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7314\n",
            "Epoch [75/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.5660\n",
            "Epoch [75/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.7440\n",
            "Epoch [75/200], Step [120/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.4802\n",
            "Epoch [75/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.9705\n",
            "Epoch [75/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7742\n",
            "Epoch [75/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7127\n",
            "Overall accuracy for this epoch:  34.17595541401274\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [76/200], Step [0/157], Time (s): 2.0, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8515\n",
            "Epoch [76/200], Step [10/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0433\n",
            "Epoch [76/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8052\n",
            "Epoch [76/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7250\n",
            "Epoch [76/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7853\n",
            "Epoch [76/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9324\n",
            "Epoch [76/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7529\n",
            "Epoch [76/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7685\n",
            "Epoch [76/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9186\n",
            "Epoch [76/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9224\n",
            "Epoch [76/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7795\n",
            "Epoch [76/200], Step [110/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.6024\n",
            "Epoch [76/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8803\n",
            "Epoch [76/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 100.0000, Loss: 1.3843\n",
            "Epoch [76/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7073\n",
            "Epoch [76/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.6937\n",
            "Overall accuracy for this epoch:  33.996815286624205\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [77/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.7206\n",
            "Epoch [77/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7657\n",
            "Epoch [77/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8453\n",
            "Epoch [77/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6733\n",
            "Epoch [77/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 100.0000, Loss: 1.4792\n",
            "Epoch [77/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8480\n",
            "Epoch [77/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7953\n",
            "Epoch [77/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.9004\n",
            "Epoch [77/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8113\n",
            "Epoch [77/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7338\n",
            "Epoch [77/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8229\n",
            "Epoch [77/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5785\n",
            "Epoch [77/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7369\n",
            "Epoch [77/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7837\n",
            "Epoch [77/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9231\n",
            "Epoch [77/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.2317\n",
            "Overall accuracy for this epoch:  33.4593949044586\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [78/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 100.0000, Loss: 1.6424\n",
            "Epoch [78/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8606\n",
            "Epoch [78/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6432\n",
            "Epoch [78/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9096\n",
            "Epoch [78/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8325\n",
            "Epoch [78/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7265\n",
            "Epoch [78/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.7760\n",
            "Epoch [78/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.0554\n",
            "Epoch [78/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 100.0000, Loss: 1.4978\n",
            "Epoch [78/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7520\n",
            "Epoch [78/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6674\n",
            "Epoch [78/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8779\n",
            "Epoch [78/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5327\n",
            "Epoch [78/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7362\n",
            "Epoch [78/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9897\n",
            "Epoch [78/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.7514\n",
            "Overall accuracy for this epoch:  33.857484076433124\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [79/200], Step [0/157], Time (s): 2.2, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8293\n",
            "Epoch [79/200], Step [10/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 84.3750, Loss: 1.5892\n",
            "Epoch [79/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.8193\n",
            "Epoch [79/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8638\n",
            "Epoch [79/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9769\n",
            "Epoch [79/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6957\n",
            "Epoch [79/200], Step [60/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.6849\n",
            "Epoch [79/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7371\n",
            "Epoch [79/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7089\n",
            "Epoch [79/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7347\n",
            "Epoch [79/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7258\n",
            "Epoch [79/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6366\n",
            "Epoch [79/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7683\n",
            "Epoch [79/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7885\n",
            "Epoch [79/200], Step [140/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 90.6250, Loss: 1.7378\n",
            "Epoch [79/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 75.0000, Loss: 1.7919\n",
            "Overall accuracy for this epoch:  34.03662420382165\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [80/200], Step [0/157], Time (s): 2.0, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6362\n",
            "Epoch [80/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8867\n",
            "Epoch [80/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5764\n",
            "Epoch [80/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 78.1250, Loss: 1.8168\n",
            "Epoch [80/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.7864\n",
            "Epoch [80/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8763\n",
            "Epoch [80/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9179\n",
            "Epoch [80/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6091\n",
            "Epoch [80/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6516\n",
            "Epoch [80/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 2.0166\n",
            "Epoch [80/200], Step [100/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.6239\n",
            "Epoch [80/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9518\n",
            "Epoch [80/200], Step [120/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7042\n",
            "Epoch [80/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.5273\n",
            "Epoch [80/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.6836\n",
            "Epoch [80/200], Step [150/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 84.3750, Loss: 1.9325\n",
            "Overall accuracy for this epoch:  33.30015923566879\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [81/200], Step [0/157], Time (s): 2.0, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0099\n",
            "Epoch [81/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6692\n",
            "Epoch [81/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9327\n",
            "Epoch [81/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8679\n",
            "Epoch [81/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 71.8750, Loss: 2.0706\n",
            "Epoch [81/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.5979\n",
            "Epoch [81/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7181\n",
            "Epoch [81/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7938\n",
            "Epoch [81/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 75.0000, Loss: 1.7194\n",
            "Epoch [81/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 2.0022\n",
            "Epoch [81/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8039\n",
            "Epoch [81/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8624\n",
            "Epoch [81/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.9670\n",
            "Epoch [81/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.9413\n",
            "Epoch [81/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.7684\n",
            "Epoch [81/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8125\n",
            "Overall accuracy for this epoch:  33.87738853503185\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [82/200], Step [0/157], Time (s): 2.0, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7064\n",
            "Epoch [82/200], Step [10/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.8069\n",
            "Epoch [82/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9303\n",
            "Epoch [82/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.1011\n",
            "Epoch [82/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6917\n",
            "Epoch [82/200], Step [50/157], Time (s): 1.1, Acc1: 62.5000, Acc5: 93.7500, Loss: 1.4439\n",
            "Epoch [82/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8950\n",
            "Epoch [82/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7405\n",
            "Epoch [82/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.6648\n",
            "Epoch [82/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8795\n",
            "Epoch [82/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9304\n",
            "Epoch [82/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.7379\n",
            "Epoch [82/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7693\n",
            "Epoch [82/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6285\n",
            "Epoch [82/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.8196\n",
            "Epoch [82/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9928\n",
            "Overall accuracy for this epoch:  33.359872611464965\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [83/200], Step [0/157], Time (s): 2.0, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9361\n",
            "Epoch [83/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8884\n",
            "Epoch [83/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.8656\n",
            "Epoch [83/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7840\n",
            "Epoch [83/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.8773\n",
            "Epoch [83/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8178\n",
            "Epoch [83/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 71.8750, Loss: 1.9480\n",
            "Epoch [83/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9236\n",
            "Epoch [83/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 100.0000, Loss: 1.4233\n",
            "Epoch [83/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7599\n",
            "Epoch [83/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.9054\n",
            "Epoch [83/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7997\n",
            "Epoch [83/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8081\n",
            "Epoch [83/200], Step [130/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.5843\n",
            "Epoch [83/200], Step [140/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.8894\n",
            "Epoch [83/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0363\n",
            "Overall accuracy for this epoch:  33.61863057324841\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [84/200], Step [0/157], Time (s): 2.0, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8300\n",
            "Epoch [84/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8906\n",
            "Epoch [84/200], Step [20/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 81.2500, Loss: 1.5495\n",
            "Epoch [84/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 2.0661\n",
            "Epoch [84/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7736\n",
            "Epoch [84/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8440\n",
            "Epoch [84/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7947\n",
            "Epoch [84/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7944\n",
            "Epoch [84/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9074\n",
            "Epoch [84/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8771\n",
            "Epoch [84/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.5648\n",
            "Epoch [84/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8972\n",
            "Epoch [84/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.9187\n",
            "Epoch [84/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.7876\n",
            "Epoch [84/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7406\n",
            "Epoch [84/200], Step [150/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.5935\n",
            "Overall accuracy for this epoch:  34.23566878980892\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [85/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.8329\n",
            "Epoch [85/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9065\n",
            "Epoch [85/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7662\n",
            "Epoch [85/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7032\n",
            "Epoch [85/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7428\n",
            "Epoch [85/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 96.8750, Loss: 1.7263\n",
            "Epoch [85/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6462\n",
            "Epoch [85/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7942\n",
            "Epoch [85/200], Step [80/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.4915\n",
            "Epoch [85/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7161\n",
            "Epoch [85/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9476\n",
            "Epoch [85/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8797\n",
            "Epoch [85/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6983\n",
            "Epoch [85/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6750\n",
            "Epoch [85/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 1.9825\n",
            "Epoch [85/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6142\n",
            "Overall accuracy for this epoch:  34.77308917197452\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [86/200], Step [0/157], Time (s): 2.0, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7486\n",
            "Epoch [86/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9671\n",
            "Epoch [86/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6203\n",
            "Epoch [86/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8735\n",
            "Epoch [86/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.8014\n",
            "Epoch [86/200], Step [50/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 90.6250, Loss: 1.5749\n",
            "Epoch [86/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.1990\n",
            "Epoch [86/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.9533\n",
            "Epoch [86/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8122\n",
            "Epoch [86/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8211\n",
            "Epoch [86/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9469\n",
            "Epoch [86/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8272\n",
            "Epoch [86/200], Step [120/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.4901\n",
            "Epoch [86/200], Step [130/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 96.8750, Loss: 1.4068\n",
            "Epoch [86/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.5884\n",
            "Epoch [86/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.1370\n",
            "Overall accuracy for this epoch:  32.84235668789809\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [87/200], Step [0/157], Time (s): 2.0, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9493\n",
            "Epoch [87/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.9826\n",
            "Epoch [87/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6769\n",
            "Epoch [87/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7617\n",
            "Epoch [87/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8727\n",
            "Epoch [87/200], Step [50/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.5541\n",
            "Epoch [87/200], Step [60/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5408\n",
            "Epoch [87/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.8293\n",
            "Epoch [87/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8617\n",
            "Epoch [87/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0242\n",
            "Epoch [87/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7792\n",
            "Epoch [87/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8810\n",
            "Epoch [87/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7592\n",
            "Epoch [87/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7111\n",
            "Epoch [87/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9754\n",
            "Epoch [87/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7941\n",
            "Overall accuracy for this epoch:  33.419585987261144\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [88/200], Step [0/157], Time (s): 2.0, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6046\n",
            "Epoch [88/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9149\n",
            "Epoch [88/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9114\n",
            "Epoch [88/200], Step [30/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.8206\n",
            "Epoch [88/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8261\n",
            "Epoch [88/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6514\n",
            "Epoch [88/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7621\n",
            "Epoch [88/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.8975\n",
            "Epoch [88/200], Step [80/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.4432\n",
            "Epoch [88/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9717\n",
            "Epoch [88/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.8200\n",
            "Epoch [88/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.2012\n",
            "Epoch [88/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.5619\n",
            "Epoch [88/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6148\n",
            "Epoch [88/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.9695\n",
            "Epoch [88/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9138\n",
            "Overall accuracy for this epoch:  33.777866242038215\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [89/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7625\n",
            "Epoch [89/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7623\n",
            "Epoch [89/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9629\n",
            "Epoch [89/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8641\n",
            "Epoch [89/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5414\n",
            "Epoch [89/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8972\n",
            "Epoch [89/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6342\n",
            "Epoch [89/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6273\n",
            "Epoch [89/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8180\n",
            "Epoch [89/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.6706\n",
            "Epoch [89/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7296\n",
            "Epoch [89/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8975\n",
            "Epoch [89/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5311\n",
            "Epoch [89/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6813\n",
            "Epoch [89/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6033\n",
            "Epoch [89/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8637\n",
            "Overall accuracy for this epoch:  33.777866242038215\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [90/200], Step [0/157], Time (s): 2.0, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6749\n",
            "Epoch [90/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.6975\n",
            "Epoch [90/200], Step [20/157], Time (s): 1.2, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8147\n",
            "Epoch [90/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7473\n",
            "Epoch [90/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6808\n",
            "Epoch [90/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7918\n",
            "Epoch [90/200], Step [60/157], Time (s): 1.1, Acc1: 9.3750, Acc5: 78.1250, Loss: 2.1323\n",
            "Epoch [90/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.9403\n",
            "Epoch [90/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8697\n",
            "Epoch [90/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6594\n",
            "Epoch [90/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6844\n",
            "Epoch [90/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7110\n",
            "Epoch [90/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6705\n",
            "Epoch [90/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.5447\n",
            "Epoch [90/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7373\n",
            "Epoch [90/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8423\n",
            "Overall accuracy for this epoch:  34.2953821656051\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [91/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7739\n",
            "Epoch [91/200], Step [10/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5600\n",
            "Epoch [91/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7758\n",
            "Epoch [91/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.6962\n",
            "Epoch [91/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6618\n",
            "Epoch [91/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0156\n",
            "Epoch [91/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.9094\n",
            "Epoch [91/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7927\n",
            "Epoch [91/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6814\n",
            "Epoch [91/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.9433\n",
            "Epoch [91/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7186\n",
            "Epoch [91/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6456\n",
            "Epoch [91/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7241\n",
            "Epoch [91/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8803\n",
            "Epoch [91/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6275\n",
            "Epoch [91/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8671\n",
            "Overall accuracy for this epoch:  34.73328025477707\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [92/200], Step [0/157], Time (s): 2.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9638\n",
            "Epoch [92/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6622\n",
            "Epoch [92/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 68.7500, Loss: 1.9962\n",
            "Epoch [92/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7560\n",
            "Epoch [92/200], Step [40/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.5577\n",
            "Epoch [92/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 90.6250, Loss: 1.7460\n",
            "Epoch [92/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7861\n",
            "Epoch [92/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7547\n",
            "Epoch [92/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7391\n",
            "Epoch [92/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8667\n",
            "Epoch [92/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9044\n",
            "Epoch [92/200], Step [110/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 96.8750, Loss: 1.5388\n",
            "Epoch [92/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8222\n",
            "Epoch [92/200], Step [130/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 90.6250, Loss: 1.8620\n",
            "Epoch [92/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7571\n",
            "Epoch [92/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8242\n",
            "Overall accuracy for this epoch:  34.892515923566876\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [93/200], Step [0/157], Time (s): 2.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.9661\n",
            "Epoch [93/200], Step [10/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7314\n",
            "Epoch [93/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6793\n",
            "Epoch [93/200], Step [30/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 93.7500, Loss: 1.4347\n",
            "Epoch [93/200], Step [40/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.8099\n",
            "Epoch [93/200], Step [50/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 90.6250, Loss: 1.9407\n",
            "Epoch [93/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.8225\n",
            "Epoch [93/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9211\n",
            "Epoch [93/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5682\n",
            "Epoch [93/200], Step [90/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 78.1250, Loss: 2.0775\n",
            "Epoch [93/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5869\n",
            "Epoch [93/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8081\n",
            "Epoch [93/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9468\n",
            "Epoch [93/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.6722\n",
            "Epoch [93/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8509\n",
            "Epoch [93/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7527\n",
            "Overall accuracy for this epoch:  34.01671974522293\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [94/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7291\n",
            "Epoch [94/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.7359\n",
            "Epoch [94/200], Step [20/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5957\n",
            "Epoch [94/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7639\n",
            "Epoch [94/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 65.6250, Loss: 2.0347\n",
            "Epoch [94/200], Step [50/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 2.2113\n",
            "Epoch [94/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6781\n",
            "Epoch [94/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7817\n",
            "Epoch [94/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8580\n",
            "Epoch [94/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.6914\n",
            "Epoch [94/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7625\n",
            "Epoch [94/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6993\n",
            "Epoch [94/200], Step [120/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 93.7500, Loss: 1.8313\n",
            "Epoch [94/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6410\n",
            "Epoch [94/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8134\n",
            "Epoch [94/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5759\n",
            "Overall accuracy for this epoch:  34.51433121019108\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [95/200], Step [0/157], Time (s): 2.1, Acc1: 53.1250, Acc5: 81.2500, Loss: 1.6703\n",
            "Epoch [95/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7998\n",
            "Epoch [95/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9016\n",
            "Epoch [95/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5749\n",
            "Epoch [95/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7521\n",
            "Epoch [95/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7534\n",
            "Epoch [95/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8999\n",
            "Epoch [95/200], Step [70/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.8580\n",
            "Epoch [95/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9436\n",
            "Epoch [95/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7839\n",
            "Epoch [95/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5899\n",
            "Epoch [95/200], Step [110/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.4410\n",
            "Epoch [95/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9420\n",
            "Epoch [95/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.7234\n",
            "Epoch [95/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8889\n",
            "Epoch [95/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.8041\n",
            "Overall accuracy for this epoch:  33.69824840764331\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [96/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.8663\n",
            "Epoch [96/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 2.0065\n",
            "Epoch [96/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8293\n",
            "Epoch [96/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8115\n",
            "Epoch [96/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8116\n",
            "Epoch [96/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6832\n",
            "Epoch [96/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7411\n",
            "Epoch [96/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8911\n",
            "Epoch [96/200], Step [80/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 68.7500, Loss: 2.1164\n",
            "Epoch [96/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 2.0493\n",
            "Epoch [96/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.7721\n",
            "Epoch [96/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7490\n",
            "Epoch [96/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8371\n",
            "Epoch [96/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7879\n",
            "Epoch [96/200], Step [140/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 96.8750, Loss: 1.4240\n",
            "Epoch [96/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8629\n",
            "Overall accuracy for this epoch:  34.63375796178344\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [97/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7505\n",
            "Epoch [97/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8881\n",
            "Epoch [97/200], Step [20/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 87.5000, Loss: 1.5448\n",
            "Epoch [97/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8943\n",
            "Epoch [97/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.6689\n",
            "Epoch [97/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8829\n",
            "Epoch [97/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7512\n",
            "Epoch [97/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6161\n",
            "Epoch [97/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8389\n",
            "Epoch [97/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8133\n",
            "Epoch [97/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7559\n",
            "Epoch [97/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.6595\n",
            "Epoch [97/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8861\n",
            "Epoch [97/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7484\n",
            "Epoch [97/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.7739\n",
            "Epoch [97/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6248\n",
            "Overall accuracy for this epoch:  34.414808917197455\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [98/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.5708\n",
            "Epoch [98/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7500\n",
            "Epoch [98/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6127\n",
            "Epoch [98/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9042\n",
            "Epoch [98/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6264\n",
            "Epoch [98/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7788\n",
            "Epoch [98/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7481\n",
            "Epoch [98/200], Step [70/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.7733\n",
            "Epoch [98/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6665\n",
            "Epoch [98/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.7251\n",
            "Epoch [98/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8077\n",
            "Epoch [98/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6973\n",
            "Epoch [98/200], Step [120/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 96.8750, Loss: 1.4620\n",
            "Epoch [98/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9297\n",
            "Epoch [98/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7294\n",
            "Epoch [98/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9753\n",
            "Overall accuracy for this epoch:  33.61863057324841\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [99/200], Step [0/157], Time (s): 2.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7923\n",
            "Epoch [99/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6375\n",
            "Epoch [99/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.8938\n",
            "Epoch [99/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8801\n",
            "Epoch [99/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5911\n",
            "Epoch [99/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7541\n",
            "Epoch [99/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 71.8750, Loss: 1.8488\n",
            "Epoch [99/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9621\n",
            "Epoch [99/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7505\n",
            "Epoch [99/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.9183\n",
            "Epoch [99/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6331\n",
            "Epoch [99/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.8652\n",
            "Epoch [99/200], Step [120/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.9131\n",
            "Epoch [99/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8346\n",
            "Epoch [99/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7114\n",
            "Epoch [99/200], Step [150/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.6120\n",
            "Overall accuracy for this epoch:  34.43471337579618\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [100/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6557\n",
            "Epoch [100/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.6520\n",
            "Epoch [100/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8517\n",
            "Epoch [100/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6046\n",
            "Epoch [100/200], Step [40/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.6165\n",
            "Epoch [100/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8363\n",
            "Epoch [100/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7857\n",
            "Epoch [100/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.7392\n",
            "Epoch [100/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9107\n",
            "Epoch [100/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7144\n",
            "Epoch [100/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.6941\n",
            "Epoch [100/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6670\n",
            "Epoch [100/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9434\n",
            "Epoch [100/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7872\n",
            "Epoch [100/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8369\n",
            "Epoch [100/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8640\n",
            "Overall accuracy for this epoch:  33.9171974522293\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [101/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.6525\n",
            "Epoch [101/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8601\n",
            "Epoch [101/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9431\n",
            "Epoch [101/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9140\n",
            "Epoch [101/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7089\n",
            "Epoch [101/200], Step [50/157], Time (s): 1.1, Acc1: 6.2500, Acc5: 84.3750, Loss: 2.1046\n",
            "Epoch [101/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.7083\n",
            "Epoch [101/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6316\n",
            "Epoch [101/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6713\n",
            "Epoch [101/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7173\n",
            "Epoch [101/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.0450\n",
            "Epoch [101/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 96.8750, Loss: 1.8172\n",
            "Epoch [101/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7485\n",
            "Epoch [101/200], Step [130/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5714\n",
            "Epoch [101/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8549\n",
            "Epoch [101/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7639\n",
            "Overall accuracy for this epoch:  34.15605095541401\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [102/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6759\n",
            "Epoch [102/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7402\n",
            "Epoch [102/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8579\n",
            "Epoch [102/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7995\n",
            "Epoch [102/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.7852\n",
            "Epoch [102/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6668\n",
            "Epoch [102/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5100\n",
            "Epoch [102/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5587\n",
            "Epoch [102/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9193\n",
            "Epoch [102/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6697\n",
            "Epoch [102/200], Step [100/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 93.7500, Loss: 1.7812\n",
            "Epoch [102/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.4956\n",
            "Epoch [102/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8628\n",
            "Epoch [102/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7703\n",
            "Epoch [102/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6649\n",
            "Epoch [102/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7783\n",
            "Overall accuracy for this epoch:  34.21576433121019\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [103/200], Step [0/157], Time (s): 2.0, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8582\n",
            "Epoch [103/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7372\n",
            "Epoch [103/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6605\n",
            "Epoch [103/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7086\n",
            "Epoch [103/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8865\n",
            "Epoch [103/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8852\n",
            "Epoch [103/200], Step [60/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.9746\n",
            "Epoch [103/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7511\n",
            "Epoch [103/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6114\n",
            "Epoch [103/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.0066\n",
            "Epoch [103/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9000\n",
            "Epoch [103/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9092\n",
            "Epoch [103/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8363\n",
            "Epoch [103/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9083\n",
            "Epoch [103/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.5549\n",
            "Epoch [103/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6634\n",
            "Overall accuracy for this epoch:  34.09633757961783\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [104/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8411\n",
            "Epoch [104/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.8725\n",
            "Epoch [104/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5382\n",
            "Epoch [104/200], Step [30/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.9391\n",
            "Epoch [104/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5902\n",
            "Epoch [104/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7641\n",
            "Epoch [104/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.5322\n",
            "Epoch [104/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6961\n",
            "Epoch [104/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.6274\n",
            "Epoch [104/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9540\n",
            "Epoch [104/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6855\n",
            "Epoch [104/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0364\n",
            "Epoch [104/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.8246\n",
            "Epoch [104/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9054\n",
            "Epoch [104/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8379\n",
            "Epoch [104/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8657\n",
            "Overall accuracy for this epoch:  33.87738853503185\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [105/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6681\n",
            "Epoch [105/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7573\n",
            "Epoch [105/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7617\n",
            "Epoch [105/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6576\n",
            "Epoch [105/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 100.0000, Loss: 1.6132\n",
            "Epoch [105/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.6096\n",
            "Epoch [105/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7787\n",
            "Epoch [105/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9283\n",
            "Epoch [105/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.9070\n",
            "Epoch [105/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7183\n",
            "Epoch [105/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6679\n",
            "Epoch [105/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8800\n",
            "Epoch [105/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.5666\n",
            "Epoch [105/200], Step [130/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.6920\n",
            "Epoch [105/200], Step [140/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.6628\n",
            "Epoch [105/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5805\n",
            "Overall accuracy for this epoch:  34.09633757961783\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [106/200], Step [0/157], Time (s): 2.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 1.9625\n",
            "Epoch [106/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7451\n",
            "Epoch [106/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6136\n",
            "Epoch [106/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7105\n",
            "Epoch [106/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7444\n",
            "Epoch [106/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6914\n",
            "Epoch [106/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.8498\n",
            "Epoch [106/200], Step [70/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8170\n",
            "Epoch [106/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 90.6250, Loss: 1.8533\n",
            "Epoch [106/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7408\n",
            "Epoch [106/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6813\n",
            "Epoch [106/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.7920\n",
            "Epoch [106/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8630\n",
            "Epoch [106/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9883\n",
            "Epoch [106/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7619\n",
            "Epoch [106/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9907\n",
            "Overall accuracy for this epoch:  33.5390127388535\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [107/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6612\n",
            "Epoch [107/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7036\n",
            "Epoch [107/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9489\n",
            "Epoch [107/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6962\n",
            "Epoch [107/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7287\n",
            "Epoch [107/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.6666\n",
            "Epoch [107/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6780\n",
            "Epoch [107/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8302\n",
            "Epoch [107/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8685\n",
            "Epoch [107/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 2.0455\n",
            "Epoch [107/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9622\n",
            "Epoch [107/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7947\n",
            "Epoch [107/200], Step [120/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.9615\n",
            "Epoch [107/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7831\n",
            "Epoch [107/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7547\n",
            "Epoch [107/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5551\n",
            "Overall accuracy for this epoch:  34.15605095541401\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [108/200], Step [0/157], Time (s): 2.2, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9772\n",
            "Epoch [108/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7391\n",
            "Epoch [108/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7627\n",
            "Epoch [108/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7874\n",
            "Epoch [108/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9220\n",
            "Epoch [108/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0031\n",
            "Epoch [108/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8455\n",
            "Epoch [108/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.5345\n",
            "Epoch [108/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7982\n",
            "Epoch [108/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.5605\n",
            "Epoch [108/200], Step [100/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.5903\n",
            "Epoch [108/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.5718\n",
            "Epoch [108/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 71.8750, Loss: 1.9121\n",
            "Epoch [108/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8673\n",
            "Epoch [108/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6054\n",
            "Epoch [108/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9178\n",
            "Overall accuracy for this epoch:  34.91242038216561\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [109/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6574\n",
            "Epoch [109/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9365\n",
            "Epoch [109/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9047\n",
            "Epoch [109/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6592\n",
            "Epoch [109/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.7534\n",
            "Epoch [109/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8161\n",
            "Epoch [109/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7492\n",
            "Epoch [109/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7300\n",
            "Epoch [109/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6983\n",
            "Epoch [109/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.5985\n",
            "Epoch [109/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0243\n",
            "Epoch [109/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 71.8750, Loss: 2.0845\n",
            "Epoch [109/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7396\n",
            "Epoch [109/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6904\n",
            "Epoch [109/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8957\n",
            "Epoch [109/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6999\n",
            "Overall accuracy for this epoch:  34.07643312101911\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [110/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6820\n",
            "Epoch [110/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6485\n",
            "Epoch [110/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9938\n",
            "Epoch [110/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9122\n",
            "Epoch [110/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8769\n",
            "Epoch [110/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8065\n",
            "Epoch [110/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6770\n",
            "Epoch [110/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7425\n",
            "Epoch [110/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8923\n",
            "Epoch [110/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.1058\n",
            "Epoch [110/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7167\n",
            "Epoch [110/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9659\n",
            "Epoch [110/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.6236\n",
            "Epoch [110/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.0027\n",
            "Epoch [110/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.5978\n",
            "Epoch [110/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7509\n",
            "Overall accuracy for this epoch:  34.25557324840764\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [111/200], Step [0/157], Time (s): 2.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.0024\n",
            "Epoch [111/200], Step [10/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7460\n",
            "Epoch [111/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7049\n",
            "Epoch [111/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9302\n",
            "Epoch [111/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8351\n",
            "Epoch [111/200], Step [50/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.8290\n",
            "Epoch [111/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9392\n",
            "Epoch [111/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8522\n",
            "Epoch [111/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8100\n",
            "Epoch [111/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9538\n",
            "Epoch [111/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6700\n",
            "Epoch [111/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.6392\n",
            "Epoch [111/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.9734\n",
            "Epoch [111/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7402\n",
            "Epoch [111/200], Step [140/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 65.6250, Loss: 2.4383\n",
            "Epoch [111/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 2.0273\n",
            "Overall accuracy for this epoch:  35.15127388535032\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [112/200], Step [0/157], Time (s): 2.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.6358\n",
            "Epoch [112/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.7818\n",
            "Epoch [112/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7356\n",
            "Epoch [112/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6898\n",
            "Epoch [112/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7269\n",
            "Epoch [112/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9054\n",
            "Epoch [112/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8261\n",
            "Epoch [112/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.8443\n",
            "Epoch [112/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.9332\n",
            "Epoch [112/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7485\n",
            "Epoch [112/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.6204\n",
            "Epoch [112/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7934\n",
            "Epoch [112/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9982\n",
            "Epoch [112/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7772\n",
            "Epoch [112/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.5316\n",
            "Epoch [112/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5555\n",
            "Overall accuracy for this epoch:  34.93232484076433\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [113/200], Step [0/157], Time (s): 2.2, Acc1: 34.3750, Acc5: 100.0000, Loss: 1.6473\n",
            "Epoch [113/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6581\n",
            "Epoch [113/200], Step [20/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.8872\n",
            "Epoch [113/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9158\n",
            "Epoch [113/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.5811\n",
            "Epoch [113/200], Step [50/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5976\n",
            "Epoch [113/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.0684\n",
            "Epoch [113/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6771\n",
            "Epoch [113/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.8027\n",
            "Epoch [113/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5955\n",
            "Epoch [113/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.1155\n",
            "Epoch [113/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7643\n",
            "Epoch [113/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 68.7500, Loss: 2.0654\n",
            "Epoch [113/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.6904\n",
            "Epoch [113/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8926\n",
            "Epoch [113/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.7188\n",
            "Overall accuracy for this epoch:  34.09633757961783\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [114/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9643\n",
            "Epoch [114/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6421\n",
            "Epoch [114/200], Step [20/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8808\n",
            "Epoch [114/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 1.9184\n",
            "Epoch [114/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.6953\n",
            "Epoch [114/200], Step [50/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 100.0000, Loss: 1.3705\n",
            "Epoch [114/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8340\n",
            "Epoch [114/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.1391\n",
            "Epoch [114/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8762\n",
            "Epoch [114/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8514\n",
            "Epoch [114/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7143\n",
            "Epoch [114/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6731\n",
            "Epoch [114/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8965\n",
            "Epoch [114/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6559\n",
            "Epoch [114/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6623\n",
            "Epoch [114/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9458\n",
            "Overall accuracy for this epoch:  34.53423566878981\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [115/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6854\n",
            "Epoch [115/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9474\n",
            "Epoch [115/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8203\n",
            "Epoch [115/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6521\n",
            "Epoch [115/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8183\n",
            "Epoch [115/200], Step [50/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.7111\n",
            "Epoch [115/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6775\n",
            "Epoch [115/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9911\n",
            "Epoch [115/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8561\n",
            "Epoch [115/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7394\n",
            "Epoch [115/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5465\n",
            "Epoch [115/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8516\n",
            "Epoch [115/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9334\n",
            "Epoch [115/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7424\n",
            "Epoch [115/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.5987\n",
            "Epoch [115/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6295\n",
            "Overall accuracy for this epoch:  34.53423566878981\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [116/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 100.0000, Loss: 1.5523\n",
            "Epoch [116/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7408\n",
            "Epoch [116/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7658\n",
            "Epoch [116/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8342\n",
            "Epoch [116/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7109\n",
            "Epoch [116/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6536\n",
            "Epoch [116/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6216\n",
            "Epoch [116/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.5724\n",
            "Epoch [116/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7163\n",
            "Epoch [116/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8288\n",
            "Epoch [116/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5083\n",
            "Epoch [116/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8542\n",
            "Epoch [116/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7925\n",
            "Epoch [116/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6888\n",
            "Epoch [116/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 75.0000, Loss: 1.8692\n",
            "Epoch [116/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8732\n",
            "Overall accuracy for this epoch:  34.53423566878981\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [117/200], Step [0/157], Time (s): 2.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.5757\n",
            "Epoch [117/200], Step [10/157], Time (s): 1.2, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8416\n",
            "Epoch [117/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6467\n",
            "Epoch [117/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7873\n",
            "Epoch [117/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6292\n",
            "Epoch [117/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 2.1152\n",
            "Epoch [117/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7139\n",
            "Epoch [117/200], Step [70/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.5840\n",
            "Epoch [117/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8942\n",
            "Epoch [117/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6862\n",
            "Epoch [117/200], Step [100/157], Time (s): 1.2, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5474\n",
            "Epoch [117/200], Step [110/157], Time (s): 1.2, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8238\n",
            "Epoch [117/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6952\n",
            "Epoch [117/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9331\n",
            "Epoch [117/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.8045\n",
            "Epoch [117/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.9926\n",
            "Overall accuracy for this epoch:  34.355095541401276\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [118/200], Step [0/157], Time (s): 2.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.7336\n",
            "Epoch [118/200], Step [10/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 81.2500, Loss: 1.9286\n",
            "Epoch [118/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7510\n",
            "Epoch [118/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8862\n",
            "Epoch [118/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7401\n",
            "Epoch [118/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.7010\n",
            "Epoch [118/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7086\n",
            "Epoch [118/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.7809\n",
            "Epoch [118/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6288\n",
            "Epoch [118/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9273\n",
            "Epoch [118/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8022\n",
            "Epoch [118/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6294\n",
            "Epoch [118/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8137\n",
            "Epoch [118/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6753\n",
            "Epoch [118/200], Step [140/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.7734\n",
            "Epoch [118/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6099\n",
            "Overall accuracy for this epoch:  34.03662420382165\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [119/200], Step [0/157], Time (s): 2.2, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7067\n",
            "Epoch [119/200], Step [10/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 2.0451\n",
            "Epoch [119/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7089\n",
            "Epoch [119/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0877\n",
            "Epoch [119/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8963\n",
            "Epoch [119/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.9293\n",
            "Epoch [119/200], Step [60/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.6563\n",
            "Epoch [119/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8187\n",
            "Epoch [119/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.6506\n",
            "Epoch [119/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8716\n",
            "Epoch [119/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8518\n",
            "Epoch [119/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8917\n",
            "Epoch [119/200], Step [120/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 78.1250, Loss: 1.7535\n",
            "Epoch [119/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5338\n",
            "Epoch [119/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.7122\n",
            "Epoch [119/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8660\n",
            "Overall accuracy for this epoch:  33.87738853503185\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [120/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.9283\n",
            "Epoch [120/200], Step [10/157], Time (s): 1.2, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.3912\n",
            "Epoch [120/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6263\n",
            "Epoch [120/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6593\n",
            "Epoch [120/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7570\n",
            "Epoch [120/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6305\n",
            "Epoch [120/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7090\n",
            "Epoch [120/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7844\n",
            "Epoch [120/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8964\n",
            "Epoch [120/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 1.9756\n",
            "Epoch [120/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7381\n",
            "Epoch [120/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7248\n",
            "Epoch [120/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6523\n",
            "Epoch [120/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7550\n",
            "Epoch [120/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9401\n",
            "Epoch [120/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8016\n",
            "Overall accuracy for this epoch:  34.53423566878981\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [121/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8278\n",
            "Epoch [121/200], Step [10/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9777\n",
            "Epoch [121/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7790\n",
            "Epoch [121/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7674\n",
            "Epoch [121/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6393\n",
            "Epoch [121/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7932\n",
            "Epoch [121/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7687\n",
            "Epoch [121/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5735\n",
            "Epoch [121/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 2.0512\n",
            "Epoch [121/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8095\n",
            "Epoch [121/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9048\n",
            "Epoch [121/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8044\n",
            "Epoch [121/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5421\n",
            "Epoch [121/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8814\n",
            "Epoch [121/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.7826\n",
            "Epoch [121/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8990\n",
            "Overall accuracy for this epoch:  34.892515923566876\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [122/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8328\n",
            "Epoch [122/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.0013\n",
            "Epoch [122/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.9455\n",
            "Epoch [122/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6812\n",
            "Epoch [122/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6945\n",
            "Epoch [122/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8693\n",
            "Epoch [122/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7124\n",
            "Epoch [122/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8365\n",
            "Epoch [122/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7277\n",
            "Epoch [122/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6774\n",
            "Epoch [122/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5255\n",
            "Epoch [122/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6025\n",
            "Epoch [122/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 71.8750, Loss: 1.9001\n",
            "Epoch [122/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9239\n",
            "Epoch [122/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8129\n",
            "Epoch [122/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7669\n",
            "Overall accuracy for this epoch:  34.15605095541401\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [123/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7701\n",
            "Epoch [123/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9484\n",
            "Epoch [123/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 2.1389\n",
            "Epoch [123/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8115\n",
            "Epoch [123/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6088\n",
            "Epoch [123/200], Step [50/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 87.5000, Loss: 1.4958\n",
            "Epoch [123/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7935\n",
            "Epoch [123/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6416\n",
            "Epoch [123/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.6985\n",
            "Epoch [123/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 71.8750, Loss: 1.7469\n",
            "Epoch [123/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7082\n",
            "Epoch [123/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6465\n",
            "Epoch [123/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8809\n",
            "Epoch [123/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6941\n",
            "Epoch [123/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7548\n",
            "Epoch [123/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6749\n",
            "Overall accuracy for this epoch:  33.996815286624205\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [124/200], Step [0/157], Time (s): 2.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8881\n",
            "Epoch [124/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6460\n",
            "Epoch [124/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9322\n",
            "Epoch [124/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8228\n",
            "Epoch [124/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7342\n",
            "Epoch [124/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7882\n",
            "Epoch [124/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7610\n",
            "Epoch [124/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7080\n",
            "Epoch [124/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 96.8750, Loss: 1.6571\n",
            "Epoch [124/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8155\n",
            "Epoch [124/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.7011\n",
            "Epoch [124/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8571\n",
            "Epoch [124/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5445\n",
            "Epoch [124/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8614\n",
            "Epoch [124/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7659\n",
            "Epoch [124/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7962\n",
            "Overall accuracy for this epoch:  34.136146496815286\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [125/200], Step [0/157], Time (s): 2.2, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7768\n",
            "Epoch [125/200], Step [10/157], Time (s): 1.1, Acc1: 62.5000, Acc5: 90.6250, Loss: 1.6329\n",
            "Epoch [125/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7388\n",
            "Epoch [125/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6717\n",
            "Epoch [125/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7394\n",
            "Epoch [125/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.8781\n",
            "Epoch [125/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6854\n",
            "Epoch [125/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8267\n",
            "Epoch [125/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7911\n",
            "Epoch [125/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8374\n",
            "Epoch [125/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.8077\n",
            "Epoch [125/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8903\n",
            "Epoch [125/200], Step [120/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 84.3750, Loss: 2.0738\n",
            "Epoch [125/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6119\n",
            "Epoch [125/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8847\n",
            "Epoch [125/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8738\n",
            "Overall accuracy for this epoch:  34.87261146496815\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [126/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8260\n",
            "Epoch [126/200], Step [10/157], Time (s): 1.2, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7947\n",
            "Epoch [126/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8793\n",
            "Epoch [126/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8843\n",
            "Epoch [126/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 68.7500, Loss: 2.0194\n",
            "Epoch [126/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7234\n",
            "Epoch [126/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8032\n",
            "Epoch [126/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7664\n",
            "Epoch [126/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 2.0132\n",
            "Epoch [126/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8992\n",
            "Epoch [126/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7863\n",
            "Epoch [126/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6619\n",
            "Epoch [126/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5826\n",
            "Epoch [126/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6318\n",
            "Epoch [126/200], Step [140/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5460\n",
            "Epoch [126/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 2.0750\n",
            "Overall accuracy for this epoch:  34.87261146496815\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [127/200], Step [0/157], Time (s): 2.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.7531\n",
            "Epoch [127/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.6406\n",
            "Epoch [127/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7440\n",
            "Epoch [127/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 68.7500, Loss: 2.0821\n",
            "Epoch [127/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8498\n",
            "Epoch [127/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7078\n",
            "Epoch [127/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8716\n",
            "Epoch [127/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7454\n",
            "Epoch [127/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5274\n",
            "Epoch [127/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.6977\n",
            "Epoch [127/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 68.7500, Loss: 2.1536\n",
            "Epoch [127/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7585\n",
            "Epoch [127/200], Step [120/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5431\n",
            "Epoch [127/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8031\n",
            "Epoch [127/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7567\n",
            "Epoch [127/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6688\n",
            "Overall accuracy for this epoch:  34.53423566878981\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [128/200], Step [0/157], Time (s): 2.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.5632\n",
            "Epoch [128/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 65.6250, Loss: 2.1388\n",
            "Epoch [128/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7335\n",
            "Epoch [128/200], Step [30/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5732\n",
            "Epoch [128/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8060\n",
            "Epoch [128/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.5580\n",
            "Epoch [128/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6156\n",
            "Epoch [128/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8037\n",
            "Epoch [128/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7007\n",
            "Epoch [128/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.7244\n",
            "Epoch [128/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 71.8750, Loss: 2.0411\n",
            "Epoch [128/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6178\n",
            "Epoch [128/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6456\n",
            "Epoch [128/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8032\n",
            "Epoch [128/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7107\n",
            "Epoch [128/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7575\n",
            "Overall accuracy for this epoch:  34.47452229299363\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [129/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8572\n",
            "Epoch [129/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.5605\n",
            "Epoch [129/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0429\n",
            "Epoch [129/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7753\n",
            "Epoch [129/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8583\n",
            "Epoch [129/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7269\n",
            "Epoch [129/200], Step [60/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5246\n",
            "Epoch [129/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8323\n",
            "Epoch [129/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.6840\n",
            "Epoch [129/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6793\n",
            "Epoch [129/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6870\n",
            "Epoch [129/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6478\n",
            "Epoch [129/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6736\n",
            "Epoch [129/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7525\n",
            "Epoch [129/200], Step [140/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 90.6250, Loss: 1.4213\n",
            "Epoch [129/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8090\n",
            "Overall accuracy for this epoch:  35.13136942675159\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [130/200], Step [0/157], Time (s): 2.2, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.8298\n",
            "Epoch [130/200], Step [10/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8577\n",
            "Epoch [130/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6640\n",
            "Epoch [130/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.8681\n",
            "Epoch [130/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8166\n",
            "Epoch [130/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7727\n",
            "Epoch [130/200], Step [60/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.6046\n",
            "Epoch [130/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8365\n",
            "Epoch [130/200], Step [80/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.4368\n",
            "Epoch [130/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7670\n",
            "Epoch [130/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7843\n",
            "Epoch [130/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7840\n",
            "Epoch [130/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.8745\n",
            "Epoch [130/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.6819\n",
            "Epoch [130/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8589\n",
            "Epoch [130/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.6609\n",
            "Overall accuracy for this epoch:  33.996815286624205\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [131/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5738\n",
            "Epoch [131/200], Step [10/157], Time (s): 1.2, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6349\n",
            "Epoch [131/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9161\n",
            "Epoch [131/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.6138\n",
            "Epoch [131/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6716\n",
            "Epoch [131/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.8535\n",
            "Epoch [131/200], Step [60/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 75.0000, Loss: 2.0872\n",
            "Epoch [131/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 2.0882\n",
            "Epoch [131/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 78.1250, Loss: 1.8258\n",
            "Epoch [131/200], Step [90/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.8332\n",
            "Epoch [131/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6985\n",
            "Epoch [131/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7028\n",
            "Epoch [131/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 68.7500, Loss: 2.0388\n",
            "Epoch [131/200], Step [130/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 75.0000, Loss: 1.7799\n",
            "Epoch [131/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.5682\n",
            "Epoch [131/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6294\n",
            "Overall accuracy for this epoch:  34.81289808917197\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [132/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7583\n",
            "Epoch [132/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.6236\n",
            "Epoch [132/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6267\n",
            "Epoch [132/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7219\n",
            "Epoch [132/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7376\n",
            "Epoch [132/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.5332\n",
            "Epoch [132/200], Step [60/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.4874\n",
            "Epoch [132/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 65.6250, Loss: 2.1565\n",
            "Epoch [132/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6578\n",
            "Epoch [132/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9895\n",
            "Epoch [132/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5848\n",
            "Epoch [132/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.8995\n",
            "Epoch [132/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7900\n",
            "Epoch [132/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7395\n",
            "Epoch [132/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.0900\n",
            "Epoch [132/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7050\n",
            "Overall accuracy for this epoch:  35.09156050955414\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [133/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7672\n",
            "Epoch [133/200], Step [10/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 71.8750, Loss: 2.0948\n",
            "Epoch [133/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7181\n",
            "Epoch [133/200], Step [30/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.6149\n",
            "Epoch [133/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9514\n",
            "Epoch [133/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.9467\n",
            "Epoch [133/200], Step [60/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.8177\n",
            "Epoch [133/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.5997\n",
            "Epoch [133/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5983\n",
            "Epoch [133/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8443\n",
            "Epoch [133/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7525\n",
            "Epoch [133/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 68.7500, Loss: 2.2557\n",
            "Epoch [133/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7039\n",
            "Epoch [133/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6312\n",
            "Epoch [133/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8137\n",
            "Epoch [133/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8282\n",
            "Overall accuracy for this epoch:  34.693471337579616\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [134/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7957\n",
            "Epoch [134/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.7961\n",
            "Epoch [134/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 2.0514\n",
            "Epoch [134/200], Step [30/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.4634\n",
            "Epoch [134/200], Step [40/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5880\n",
            "Epoch [134/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8265\n",
            "Epoch [134/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7115\n",
            "Epoch [134/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.6606\n",
            "Epoch [134/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7797\n",
            "Epoch [134/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7306\n",
            "Epoch [134/200], Step [100/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.4081\n",
            "Epoch [134/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7115\n",
            "Epoch [134/200], Step [120/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5803\n",
            "Epoch [134/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8572\n",
            "Epoch [134/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.6557\n",
            "Epoch [134/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6419\n",
            "Overall accuracy for this epoch:  35.25079617834395\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [135/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.4827\n",
            "Epoch [135/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.4699\n",
            "Epoch [135/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7352\n",
            "Epoch [135/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.9125\n",
            "Epoch [135/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7751\n",
            "Epoch [135/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9421\n",
            "Epoch [135/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8838\n",
            "Epoch [135/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7023\n",
            "Epoch [135/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7920\n",
            "Epoch [135/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7061\n",
            "Epoch [135/200], Step [100/157], Time (s): 1.1, Acc1: 12.5000, Acc5: 81.2500, Loss: 1.9802\n",
            "Epoch [135/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7153\n",
            "Epoch [135/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7308\n",
            "Epoch [135/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5586\n",
            "Epoch [135/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8656\n",
            "Epoch [135/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 75.0000, Loss: 1.8892\n",
            "Overall accuracy for this epoch:  34.93232484076433\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [136/200], Step [0/157], Time (s): 2.2, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.4130\n",
            "Epoch [136/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7482\n",
            "Epoch [136/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7623\n",
            "Epoch [136/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.8857\n",
            "Epoch [136/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.8003\n",
            "Epoch [136/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.7161\n",
            "Epoch [136/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6678\n",
            "Epoch [136/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8071\n",
            "Epoch [136/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7390\n",
            "Epoch [136/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.6613\n",
            "Epoch [136/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.8126\n",
            "Epoch [136/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6843\n",
            "Epoch [136/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6805\n",
            "Epoch [136/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7220\n",
            "Epoch [136/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8187\n",
            "Epoch [136/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5970\n",
            "Overall accuracy for this epoch:  34.693471337579616\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [137/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7077\n",
            "Epoch [137/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6020\n",
            "Epoch [137/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7634\n",
            "Epoch [137/200], Step [30/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.6066\n",
            "Epoch [137/200], Step [40/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 93.7500, Loss: 1.9209\n",
            "Epoch [137/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8870\n",
            "Epoch [137/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6308\n",
            "Epoch [137/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9789\n",
            "Epoch [137/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8642\n",
            "Epoch [137/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6738\n",
            "Epoch [137/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7529\n",
            "Epoch [137/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.7881\n",
            "Epoch [137/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8224\n",
            "Epoch [137/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7857\n",
            "Epoch [137/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6515\n",
            "Epoch [137/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 2.1188\n",
            "Overall accuracy for this epoch:  34.394904458598724\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [138/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7206\n",
            "Epoch [138/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9354\n",
            "Epoch [138/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.7979\n",
            "Epoch [138/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 2.1055\n",
            "Epoch [138/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8893\n",
            "Epoch [138/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9730\n",
            "Epoch [138/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6412\n",
            "Epoch [138/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7554\n",
            "Epoch [138/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.9843\n",
            "Epoch [138/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6480\n",
            "Epoch [138/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7795\n",
            "Epoch [138/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.8294\n",
            "Epoch [138/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7892\n",
            "Epoch [138/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7518\n",
            "Epoch [138/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7414\n",
            "Epoch [138/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7145\n",
            "Overall accuracy for this epoch:  35.27070063694268\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [139/200], Step [0/157], Time (s): 2.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5976\n",
            "Epoch [139/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 65.6250, Loss: 1.9513\n",
            "Epoch [139/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7569\n",
            "Epoch [139/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7391\n",
            "Epoch [139/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8660\n",
            "Epoch [139/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8046\n",
            "Epoch [139/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9161\n",
            "Epoch [139/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6742\n",
            "Epoch [139/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 68.7500, Loss: 2.5075\n",
            "Epoch [139/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7605\n",
            "Epoch [139/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7790\n",
            "Epoch [139/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.8219\n",
            "Epoch [139/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.0673\n",
            "Epoch [139/200], Step [130/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 2.1036\n",
            "Epoch [139/200], Step [140/157], Time (s): 1.2, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8572\n",
            "Epoch [139/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.9224\n",
            "Overall accuracy for this epoch:  34.43471337579618\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [140/200], Step [0/157], Time (s): 2.0, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8270\n",
            "Epoch [140/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6826\n",
            "Epoch [140/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7182\n",
            "Epoch [140/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0505\n",
            "Epoch [140/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.9084\n",
            "Epoch [140/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 100.0000, Loss: 1.3651\n",
            "Epoch [140/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7108\n",
            "Epoch [140/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5845\n",
            "Epoch [140/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8135\n",
            "Epoch [140/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6816\n",
            "Epoch [140/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5466\n",
            "Epoch [140/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6597\n",
            "Epoch [140/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8015\n",
            "Epoch [140/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9316\n",
            "Epoch [140/200], Step [140/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5844\n",
            "Epoch [140/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5284\n",
            "Overall accuracy for this epoch:  34.77308917197452\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [141/200], Step [0/157], Time (s): 2.0, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9058\n",
            "Epoch [141/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.7872\n",
            "Epoch [141/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6991\n",
            "Epoch [141/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8436\n",
            "Epoch [141/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6909\n",
            "Epoch [141/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9341\n",
            "Epoch [141/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7757\n",
            "Epoch [141/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5298\n",
            "Epoch [141/200], Step [80/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.8554\n",
            "Epoch [141/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.6010\n",
            "Epoch [141/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 2.0027\n",
            "Epoch [141/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8420\n",
            "Epoch [141/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7607\n",
            "Epoch [141/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 2.0788\n",
            "Epoch [141/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7648\n",
            "Epoch [141/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8678\n",
            "Overall accuracy for this epoch:  34.99203821656051\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [142/200], Step [0/157], Time (s): 2.2, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6166\n",
            "Epoch [142/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7268\n",
            "Epoch [142/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7499\n",
            "Epoch [142/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5419\n",
            "Epoch [142/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.6599\n",
            "Epoch [142/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7396\n",
            "Epoch [142/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8585\n",
            "Epoch [142/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7185\n",
            "Epoch [142/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.8053\n",
            "Epoch [142/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 2.1057\n",
            "Epoch [142/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6894\n",
            "Epoch [142/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7631\n",
            "Epoch [142/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8939\n",
            "Epoch [142/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7510\n",
            "Epoch [142/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5693\n",
            "Epoch [142/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9108\n",
            "Overall accuracy for this epoch:  33.797770700636946\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [143/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8766\n",
            "Epoch [143/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.4325\n",
            "Epoch [143/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8439\n",
            "Epoch [143/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8344\n",
            "Epoch [143/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7174\n",
            "Epoch [143/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6100\n",
            "Epoch [143/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6311\n",
            "Epoch [143/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.7367\n",
            "Epoch [143/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8110\n",
            "Epoch [143/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7019\n",
            "Epoch [143/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9382\n",
            "Epoch [143/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7533\n",
            "Epoch [143/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9998\n",
            "Epoch [143/200], Step [130/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 81.2500, Loss: 1.8826\n",
            "Epoch [143/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8285\n",
            "Epoch [143/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7016\n",
            "Overall accuracy for this epoch:  34.8328025477707\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [144/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.8250\n",
            "Epoch [144/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8117\n",
            "Epoch [144/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7740\n",
            "Epoch [144/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5772\n",
            "Epoch [144/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6589\n",
            "Epoch [144/200], Step [50/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.4812\n",
            "Epoch [144/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8454\n",
            "Epoch [144/200], Step [70/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.6788\n",
            "Epoch [144/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 100.0000, Loss: 1.6299\n",
            "Epoch [144/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7788\n",
            "Epoch [144/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6749\n",
            "Epoch [144/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6584\n",
            "Epoch [144/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6109\n",
            "Epoch [144/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8877\n",
            "Epoch [144/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8985\n",
            "Epoch [144/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5191\n",
            "Overall accuracy for this epoch:  34.335191082802545\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [145/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7813\n",
            "Epoch [145/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7158\n",
            "Epoch [145/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7676\n",
            "Epoch [145/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7689\n",
            "Epoch [145/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9840\n",
            "Epoch [145/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7977\n",
            "Epoch [145/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.2152\n",
            "Epoch [145/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6378\n",
            "Epoch [145/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9760\n",
            "Epoch [145/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7253\n",
            "Epoch [145/200], Step [100/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.8000\n",
            "Epoch [145/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7348\n",
            "Epoch [145/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.7553\n",
            "Epoch [145/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9496\n",
            "Epoch [145/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7871\n",
            "Epoch [145/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6488\n",
            "Overall accuracy for this epoch:  35.2906050955414\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [146/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9335\n",
            "Epoch [146/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.0156\n",
            "Epoch [146/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.3373\n",
            "Epoch [146/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.6788\n",
            "Epoch [146/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7194\n",
            "Epoch [146/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.1630\n",
            "Epoch [146/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8317\n",
            "Epoch [146/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.4904\n",
            "Epoch [146/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.7868\n",
            "Epoch [146/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8977\n",
            "Epoch [146/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7056\n",
            "Epoch [146/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5889\n",
            "Epoch [146/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.6043\n",
            "Epoch [146/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 1.9692\n",
            "Epoch [146/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 2.0348\n",
            "Epoch [146/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 2.0393\n",
            "Overall accuracy for this epoch:  35.13136942675159\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [147/200], Step [0/157], Time (s): 2.2, Acc1: 34.3750, Acc5: 75.0000, Loss: 2.0136\n",
            "Epoch [147/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6025\n",
            "Epoch [147/200], Step [20/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 87.5000, Loss: 2.0002\n",
            "Epoch [147/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8624\n",
            "Epoch [147/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7575\n",
            "Epoch [147/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6355\n",
            "Epoch [147/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8461\n",
            "Epoch [147/200], Step [70/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 90.6250, Loss: 1.5477\n",
            "Epoch [147/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7346\n",
            "Epoch [147/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.1059\n",
            "Epoch [147/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7453\n",
            "Epoch [147/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8674\n",
            "Epoch [147/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9460\n",
            "Epoch [147/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7607\n",
            "Epoch [147/200], Step [140/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 65.6250, Loss: 2.3040\n",
            "Epoch [147/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 90.6250, Loss: 1.8955\n",
            "Overall accuracy for this epoch:  33.51910828025478\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [148/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6286\n",
            "Epoch [148/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8828\n",
            "Epoch [148/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.1624\n",
            "Epoch [148/200], Step [30/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.6580\n",
            "Epoch [148/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8316\n",
            "Epoch [148/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7460\n",
            "Epoch [148/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5104\n",
            "Epoch [148/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.4562\n",
            "Epoch [148/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.6544\n",
            "Epoch [148/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.6810\n",
            "Epoch [148/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6251\n",
            "Epoch [148/200], Step [110/157], Time (s): 1.2, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7650\n",
            "Epoch [148/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 2.0661\n",
            "Epoch [148/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8628\n",
            "Epoch [148/200], Step [140/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 2.1780\n",
            "Epoch [148/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7331\n",
            "Overall accuracy for this epoch:  34.2953821656051\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [149/200], Step [0/157], Time (s): 2.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.7605\n",
            "Epoch [149/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7607\n",
            "Epoch [149/200], Step [20/157], Time (s): 1.2, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6351\n",
            "Epoch [149/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7476\n",
            "Epoch [149/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6794\n",
            "Epoch [149/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9938\n",
            "Epoch [149/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7365\n",
            "Epoch [149/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9849\n",
            "Epoch [149/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.4836\n",
            "Epoch [149/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8162\n",
            "Epoch [149/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.9927\n",
            "Epoch [149/200], Step [110/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 78.1250, Loss: 1.6562\n",
            "Epoch [149/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7627\n",
            "Epoch [149/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6386\n",
            "Epoch [149/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9527\n",
            "Epoch [149/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6726\n",
            "Overall accuracy for this epoch:  34.59394904458599\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [150/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.7117\n",
            "Epoch [150/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7976\n",
            "Epoch [150/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8240\n",
            "Epoch [150/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7168\n",
            "Epoch [150/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7052\n",
            "Epoch [150/200], Step [50/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 96.8750, Loss: 1.8012\n",
            "Epoch [150/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7358\n",
            "Epoch [150/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.5203\n",
            "Epoch [150/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.5627\n",
            "Epoch [150/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7471\n",
            "Epoch [150/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.7267\n",
            "Epoch [150/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8683\n",
            "Epoch [150/200], Step [120/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 1.9750\n",
            "Epoch [150/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7282\n",
            "Epoch [150/200], Step [140/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8194\n",
            "Epoch [150/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8086\n",
            "Overall accuracy for this epoch:  34.17595541401274\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [151/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6620\n",
            "Epoch [151/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7168\n",
            "Epoch [151/200], Step [20/157], Time (s): 1.2, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5896\n",
            "Epoch [151/200], Step [30/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 96.8750, Loss: 1.4477\n",
            "Epoch [151/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7729\n",
            "Epoch [151/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.9606\n",
            "Epoch [151/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 2.1256\n",
            "Epoch [151/200], Step [70/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.9160\n",
            "Epoch [151/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7707\n",
            "Epoch [151/200], Step [90/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.7362\n",
            "Epoch [151/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.9875\n",
            "Epoch [151/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7508\n",
            "Epoch [151/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6485\n",
            "Epoch [151/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.6824\n",
            "Epoch [151/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 2.0069\n",
            "Epoch [151/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7695\n",
            "Overall accuracy for this epoch:  35.50955414012739\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [152/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 71.8750, Loss: 1.9070\n",
            "Epoch [152/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8322\n",
            "Epoch [152/200], Step [20/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8637\n",
            "Epoch [152/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7798\n",
            "Epoch [152/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7089\n",
            "Epoch [152/200], Step [50/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 1.8811\n",
            "Epoch [152/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7148\n",
            "Epoch [152/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.9613\n",
            "Epoch [152/200], Step [80/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.7049\n",
            "Epoch [152/200], Step [90/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 78.1250, Loss: 1.9477\n",
            "Epoch [152/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.9335\n",
            "Epoch [152/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9064\n",
            "Epoch [152/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8052\n",
            "Epoch [152/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7975\n",
            "Epoch [152/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7554\n",
            "Epoch [152/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7299\n",
            "Overall accuracy for this epoch:  34.2953821656051\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [153/200], Step [0/157], Time (s): 2.2, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7461\n",
            "Epoch [153/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8071\n",
            "Epoch [153/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6029\n",
            "Epoch [153/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8975\n",
            "Epoch [153/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7572\n",
            "Epoch [153/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7025\n",
            "Epoch [153/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8315\n",
            "Epoch [153/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.5797\n",
            "Epoch [153/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6495\n",
            "Epoch [153/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9321\n",
            "Epoch [153/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7376\n",
            "Epoch [153/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8764\n",
            "Epoch [153/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8505\n",
            "Epoch [153/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8636\n",
            "Epoch [153/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8958\n",
            "Epoch [153/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8520\n",
            "Overall accuracy for this epoch:  34.99203821656051\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [154/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9281\n",
            "Epoch [154/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6433\n",
            "Epoch [154/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8449\n",
            "Epoch [154/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6000\n",
            "Epoch [154/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6334\n",
            "Epoch [154/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.1184\n",
            "Epoch [154/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8495\n",
            "Epoch [154/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6924\n",
            "Epoch [154/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 2.0519\n",
            "Epoch [154/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8177\n",
            "Epoch [154/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7199\n",
            "Epoch [154/200], Step [110/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 93.7500, Loss: 1.4970\n",
            "Epoch [154/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8952\n",
            "Epoch [154/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9829\n",
            "Epoch [154/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9652\n",
            "Epoch [154/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 78.1250, Loss: 1.6243\n",
            "Overall accuracy for this epoch:  34.53423566878981\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [155/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7758\n",
            "Epoch [155/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7212\n",
            "Epoch [155/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7114\n",
            "Epoch [155/200], Step [30/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 90.6250, Loss: 1.8760\n",
            "Epoch [155/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6133\n",
            "Epoch [155/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8536\n",
            "Epoch [155/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.8779\n",
            "Epoch [155/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7791\n",
            "Epoch [155/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8210\n",
            "Epoch [155/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 75.0000, Loss: 2.1709\n",
            "Epoch [155/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7756\n",
            "Epoch [155/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8179\n",
            "Epoch [155/200], Step [120/157], Time (s): 1.2, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8606\n",
            "Epoch [155/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8444\n",
            "Epoch [155/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8982\n",
            "Epoch [155/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8618\n",
            "Overall accuracy for this epoch:  34.394904458598724\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [156/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7712\n",
            "Epoch [156/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5669\n",
            "Epoch [156/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7563\n",
            "Epoch [156/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8132\n",
            "Epoch [156/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8063\n",
            "Epoch [156/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 2.1664\n",
            "Epoch [156/200], Step [60/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 84.3750, Loss: 1.8202\n",
            "Epoch [156/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8960\n",
            "Epoch [156/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8549\n",
            "Epoch [156/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6748\n",
            "Epoch [156/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9657\n",
            "Epoch [156/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0186\n",
            "Epoch [156/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5952\n",
            "Epoch [156/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9824\n",
            "Epoch [156/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.8177\n",
            "Epoch [156/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7570\n",
            "Overall accuracy for this epoch:  34.77308917197452\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [157/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.8299\n",
            "Epoch [157/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7888\n",
            "Epoch [157/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 68.7500, Loss: 2.0239\n",
            "Epoch [157/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7898\n",
            "Epoch [157/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8314\n",
            "Epoch [157/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 2.0438\n",
            "Epoch [157/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6057\n",
            "Epoch [157/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.9779\n",
            "Epoch [157/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.4255\n",
            "Epoch [157/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9162\n",
            "Epoch [157/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9684\n",
            "Epoch [157/200], Step [110/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.6836\n",
            "Epoch [157/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7143\n",
            "Epoch [157/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.5869\n",
            "Epoch [157/200], Step [140/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 78.1250, Loss: 1.7880\n",
            "Epoch [157/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5825\n",
            "Overall accuracy for this epoch:  34.8328025477707\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [158/200], Step [0/157], Time (s): 2.1, Acc1: 21.8750, Acc5: 75.0000, Loss: 1.9983\n",
            "Epoch [158/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7967\n",
            "Epoch [158/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8979\n",
            "Epoch [158/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6583\n",
            "Epoch [158/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.6759\n",
            "Epoch [158/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7946\n",
            "Epoch [158/200], Step [60/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 2.0757\n",
            "Epoch [158/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.6583\n",
            "Epoch [158/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.8986\n",
            "Epoch [158/200], Step [90/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 81.2500, Loss: 1.6222\n",
            "Epoch [158/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8466\n",
            "Epoch [158/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7323\n",
            "Epoch [158/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.0897\n",
            "Epoch [158/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8909\n",
            "Epoch [158/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6880\n",
            "Epoch [158/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9082\n",
            "Overall accuracy for this epoch:  34.554140127388536\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [159/200], Step [0/157], Time (s): 2.2, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9694\n",
            "Epoch [159/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.6681\n",
            "Epoch [159/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6733\n",
            "Epoch [159/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9159\n",
            "Epoch [159/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.7706\n",
            "Epoch [159/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8868\n",
            "Epoch [159/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6995\n",
            "Epoch [159/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9212\n",
            "Epoch [159/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7213\n",
            "Epoch [159/200], Step [90/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 96.8750, Loss: 1.4884\n",
            "Epoch [159/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6216\n",
            "Epoch [159/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.6337\n",
            "Epoch [159/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.9299\n",
            "Epoch [159/200], Step [130/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.7750\n",
            "Epoch [159/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 68.7500, Loss: 2.1318\n",
            "Epoch [159/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7277\n",
            "Overall accuracy for this epoch:  34.355095541401276\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [160/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.0130\n",
            "Epoch [160/200], Step [10/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6075\n",
            "Epoch [160/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 2.0097\n",
            "Epoch [160/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7856\n",
            "Epoch [160/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 1.8836\n",
            "Epoch [160/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7110\n",
            "Epoch [160/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5379\n",
            "Epoch [160/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.5893\n",
            "Epoch [160/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9715\n",
            "Epoch [160/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6402\n",
            "Epoch [160/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8554\n",
            "Epoch [160/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.9072\n",
            "Epoch [160/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7716\n",
            "Epoch [160/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.6849\n",
            "Epoch [160/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5130\n",
            "Epoch [160/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8666\n",
            "Overall accuracy for this epoch:  34.65366242038217\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [161/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7536\n",
            "Epoch [161/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8573\n",
            "Epoch [161/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9790\n",
            "Epoch [161/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.5115\n",
            "Epoch [161/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6078\n",
            "Epoch [161/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8705\n",
            "Epoch [161/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.5865\n",
            "Epoch [161/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6608\n",
            "Epoch [161/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8318\n",
            "Epoch [161/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5285\n",
            "Epoch [161/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7075\n",
            "Epoch [161/200], Step [110/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 1.9493\n",
            "Epoch [161/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.8716\n",
            "Epoch [161/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7724\n",
            "Epoch [161/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.6415\n",
            "Epoch [161/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9238\n",
            "Overall accuracy for this epoch:  35.23089171974522\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [162/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.5331\n",
            "Epoch [162/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 2.0355\n",
            "Epoch [162/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 2.0920\n",
            "Epoch [162/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7961\n",
            "Epoch [162/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5510\n",
            "Epoch [162/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7084\n",
            "Epoch [162/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7289\n",
            "Epoch [162/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6577\n",
            "Epoch [162/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8152\n",
            "Epoch [162/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 71.8750, Loss: 1.8070\n",
            "Epoch [162/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6250\n",
            "Epoch [162/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9411\n",
            "Epoch [162/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6429\n",
            "Epoch [162/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.1250\n",
            "Epoch [162/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8584\n",
            "Epoch [162/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8965\n",
            "Overall accuracy for this epoch:  34.71337579617835\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [163/200], Step [0/157], Time (s): 2.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7733\n",
            "Epoch [163/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8554\n",
            "Epoch [163/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.7902\n",
            "Epoch [163/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9116\n",
            "Epoch [163/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.7009\n",
            "Epoch [163/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8173\n",
            "Epoch [163/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.5876\n",
            "Epoch [163/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.7802\n",
            "Epoch [163/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8230\n",
            "Epoch [163/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6914\n",
            "Epoch [163/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.8360\n",
            "Epoch [163/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.9649\n",
            "Epoch [163/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 100.0000, Loss: 1.5701\n",
            "Epoch [163/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 71.8750, Loss: 1.8096\n",
            "Epoch [163/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 2.0070\n",
            "Epoch [163/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8818\n",
            "Overall accuracy for this epoch:  34.99203821656051\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [164/200], Step [0/157], Time (s): 2.2, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7067\n",
            "Epoch [164/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5042\n",
            "Epoch [164/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7035\n",
            "Epoch [164/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6237\n",
            "Epoch [164/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8089\n",
            "Epoch [164/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7694\n",
            "Epoch [164/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8162\n",
            "Epoch [164/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 2.1000\n",
            "Epoch [164/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.6687\n",
            "Epoch [164/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 68.7500, Loss: 2.1245\n",
            "Epoch [164/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7186\n",
            "Epoch [164/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8752\n",
            "Epoch [164/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7987\n",
            "Epoch [164/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7421\n",
            "Epoch [164/200], Step [140/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9081\n",
            "Epoch [164/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6234\n",
            "Overall accuracy for this epoch:  34.335191082802545\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [165/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.5528\n",
            "Epoch [165/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8261\n",
            "Epoch [165/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6771\n",
            "Epoch [165/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8736\n",
            "Epoch [165/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8284\n",
            "Epoch [165/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6667\n",
            "Epoch [165/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8873\n",
            "Epoch [165/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8196\n",
            "Epoch [165/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7199\n",
            "Epoch [165/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9919\n",
            "Epoch [165/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 2.0370\n",
            "Epoch [165/200], Step [110/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5531\n",
            "Epoch [165/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5389\n",
            "Epoch [165/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.8881\n",
            "Epoch [165/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5199\n",
            "Epoch [165/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7860\n",
            "Overall accuracy for this epoch:  34.952229299363054\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [166/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7852\n",
            "Epoch [166/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 1.9804\n",
            "Epoch [166/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6777\n",
            "Epoch [166/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7071\n",
            "Epoch [166/200], Step [40/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.6464\n",
            "Epoch [166/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7113\n",
            "Epoch [166/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8770\n",
            "Epoch [166/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.6825\n",
            "Epoch [166/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6449\n",
            "Epoch [166/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8191\n",
            "Epoch [166/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7143\n",
            "Epoch [166/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6529\n",
            "Epoch [166/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7457\n",
            "Epoch [166/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 2.0157\n",
            "Epoch [166/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7116\n",
            "Epoch [166/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8009\n",
            "Overall accuracy for this epoch:  35.2906050955414\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [167/200], Step [0/157], Time (s): 2.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6990\n",
            "Epoch [167/200], Step [10/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7507\n",
            "Epoch [167/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7706\n",
            "Epoch [167/200], Step [30/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 2.1811\n",
            "Epoch [167/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6238\n",
            "Epoch [167/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8732\n",
            "Epoch [167/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 2.0081\n",
            "Epoch [167/200], Step [70/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 84.3750, Loss: 1.9321\n",
            "Epoch [167/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7961\n",
            "Epoch [167/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7039\n",
            "Epoch [167/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8309\n",
            "Epoch [167/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6863\n",
            "Epoch [167/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8442\n",
            "Epoch [167/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6915\n",
            "Epoch [167/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.7683\n",
            "Epoch [167/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.6409\n",
            "Overall accuracy for this epoch:  34.892515923566876\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [168/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8196\n",
            "Epoch [168/200], Step [10/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.5212\n",
            "Epoch [168/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6820\n",
            "Epoch [168/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7122\n",
            "Epoch [168/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.7972\n",
            "Epoch [168/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6012\n",
            "Epoch [168/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.6314\n",
            "Epoch [168/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.8847\n",
            "Epoch [168/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6870\n",
            "Epoch [168/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9837\n",
            "Epoch [168/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8586\n",
            "Epoch [168/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7447\n",
            "Epoch [168/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6361\n",
            "Epoch [168/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8860\n",
            "Epoch [168/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 2.0175\n",
            "Epoch [168/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.7736\n",
            "Overall accuracy for this epoch:  35.6687898089172\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [169/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7604\n",
            "Epoch [169/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7678\n",
            "Epoch [169/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6854\n",
            "Epoch [169/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8671\n",
            "Epoch [169/200], Step [40/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.4444\n",
            "Epoch [169/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6811\n",
            "Epoch [169/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7121\n",
            "Epoch [169/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8197\n",
            "Epoch [169/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7753\n",
            "Epoch [169/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7078\n",
            "Epoch [169/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7326\n",
            "Epoch [169/200], Step [110/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.7924\n",
            "Epoch [169/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8544\n",
            "Epoch [169/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7229\n",
            "Epoch [169/200], Step [140/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 84.3750, Loss: 1.7206\n",
            "Epoch [169/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.5049\n",
            "Overall accuracy for this epoch:  34.414808917197455\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [170/200], Step [0/157], Time (s): 2.2, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8005\n",
            "Epoch [170/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8689\n",
            "Epoch [170/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6073\n",
            "Epoch [170/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6756\n",
            "Epoch [170/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5336\n",
            "Epoch [170/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8589\n",
            "Epoch [170/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 2.0354\n",
            "Epoch [170/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7406\n",
            "Epoch [170/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8633\n",
            "Epoch [170/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7496\n",
            "Epoch [170/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8143\n",
            "Epoch [170/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8110\n",
            "Epoch [170/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9118\n",
            "Epoch [170/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7230\n",
            "Epoch [170/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6597\n",
            "Epoch [170/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7055\n",
            "Overall accuracy for this epoch:  34.87261146496815\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [171/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.8853\n",
            "Epoch [171/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7997\n",
            "Epoch [171/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7914\n",
            "Epoch [171/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7757\n",
            "Epoch [171/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6992\n",
            "Epoch [171/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.6938\n",
            "Epoch [171/200], Step [60/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9166\n",
            "Epoch [171/200], Step [70/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 78.1250, Loss: 1.6182\n",
            "Epoch [171/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6966\n",
            "Epoch [171/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6668\n",
            "Epoch [171/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.9163\n",
            "Epoch [171/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8555\n",
            "Epoch [171/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8572\n",
            "Epoch [171/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.5449\n",
            "Epoch [171/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.5519\n",
            "Epoch [171/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9893\n",
            "Overall accuracy for this epoch:  34.43471337579618\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [172/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8265\n",
            "Epoch [172/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5364\n",
            "Epoch [172/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 96.8750, Loss: 1.6205\n",
            "Epoch [172/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8139\n",
            "Epoch [172/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.5413\n",
            "Epoch [172/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9473\n",
            "Epoch [172/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7496\n",
            "Epoch [172/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.7485\n",
            "Epoch [172/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6650\n",
            "Epoch [172/200], Step [90/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.5169\n",
            "Epoch [172/200], Step [100/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 2.0011\n",
            "Epoch [172/200], Step [110/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.5871\n",
            "Epoch [172/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.5410\n",
            "Epoch [172/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6267\n",
            "Epoch [172/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5654\n",
            "Epoch [172/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7998\n",
            "Overall accuracy for this epoch:  35.15127388535032\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [173/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7093\n",
            "Epoch [173/200], Step [10/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 100.0000, Loss: 1.5931\n",
            "Epoch [173/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6023\n",
            "Epoch [173/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8776\n",
            "Epoch [173/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7257\n",
            "Epoch [173/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.7590\n",
            "Epoch [173/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 2.0041\n",
            "Epoch [173/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7938\n",
            "Epoch [173/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 71.8750, Loss: 1.9754\n",
            "Epoch [173/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6075\n",
            "Epoch [173/200], Step [100/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5943\n",
            "Epoch [173/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7593\n",
            "Epoch [173/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.9712\n",
            "Epoch [173/200], Step [130/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 90.6250, Loss: 1.8819\n",
            "Epoch [173/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8916\n",
            "Epoch [173/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8720\n",
            "Overall accuracy for this epoch:  34.65366242038217\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [174/200], Step [0/157], Time (s): 2.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9195\n",
            "Epoch [174/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6629\n",
            "Epoch [174/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.0253\n",
            "Epoch [174/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6290\n",
            "Epoch [174/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.5950\n",
            "Epoch [174/200], Step [50/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.5635\n",
            "Epoch [174/200], Step [60/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6388\n",
            "Epoch [174/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8196\n",
            "Epoch [174/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 71.8750, Loss: 1.9615\n",
            "Epoch [174/200], Step [90/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9158\n",
            "Epoch [174/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7536\n",
            "Epoch [174/200], Step [110/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.8528\n",
            "Epoch [174/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7924\n",
            "Epoch [174/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6589\n",
            "Epoch [174/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7366\n",
            "Epoch [174/200], Step [150/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.7599\n",
            "Overall accuracy for this epoch:  34.65366242038217\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [175/200], Step [0/157], Time (s): 2.2, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.4353\n",
            "Epoch [175/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.7977\n",
            "Epoch [175/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 96.8750, Loss: 1.6575\n",
            "Epoch [175/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.5938\n",
            "Epoch [175/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.7885\n",
            "Epoch [175/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6453\n",
            "Epoch [175/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9966\n",
            "Epoch [175/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 78.1250, Loss: 1.9131\n",
            "Epoch [175/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6788\n",
            "Epoch [175/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8601\n",
            "Epoch [175/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7461\n",
            "Epoch [175/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7186\n",
            "Epoch [175/200], Step [120/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.7750\n",
            "Epoch [175/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8718\n",
            "Epoch [175/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 81.2500, Loss: 1.7720\n",
            "Epoch [175/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6135\n",
            "Overall accuracy for this epoch:  34.67356687898089\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [176/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9726\n",
            "Epoch [176/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.6407\n",
            "Epoch [176/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8972\n",
            "Epoch [176/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7266\n",
            "Epoch [176/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7936\n",
            "Epoch [176/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 78.1250, Loss: 1.9054\n",
            "Epoch [176/200], Step [60/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.4437\n",
            "Epoch [176/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8711\n",
            "Epoch [176/200], Step [80/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 78.1250, Loss: 1.6767\n",
            "Epoch [176/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8175\n",
            "Epoch [176/200], Step [100/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5007\n",
            "Epoch [176/200], Step [110/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 71.8750, Loss: 2.0585\n",
            "Epoch [176/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.7697\n",
            "Epoch [176/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7309\n",
            "Epoch [176/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.7394\n",
            "Epoch [176/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.8101\n",
            "Overall accuracy for this epoch:  34.91242038216561\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [177/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 75.0000, Loss: 1.7920\n",
            "Epoch [177/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.5795\n",
            "Epoch [177/200], Step [20/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.5769\n",
            "Epoch [177/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9789\n",
            "Epoch [177/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7284\n",
            "Epoch [177/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6960\n",
            "Epoch [177/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7524\n",
            "Epoch [177/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8077\n",
            "Epoch [177/200], Step [80/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.5546\n",
            "Epoch [177/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7477\n",
            "Epoch [177/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.7248\n",
            "Epoch [177/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6433\n",
            "Epoch [177/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8416\n",
            "Epoch [177/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.8793\n",
            "Epoch [177/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8780\n",
            "Epoch [177/200], Step [150/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.8353\n",
            "Overall accuracy for this epoch:  34.85270700636943\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [178/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9327\n",
            "Epoch [178/200], Step [10/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 96.8750, Loss: 1.3907\n",
            "Epoch [178/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6504\n",
            "Epoch [178/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9488\n",
            "Epoch [178/200], Step [40/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8436\n",
            "Epoch [178/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7896\n",
            "Epoch [178/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6655\n",
            "Epoch [178/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8970\n",
            "Epoch [178/200], Step [80/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.4815\n",
            "Epoch [178/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.6873\n",
            "Epoch [178/200], Step [100/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 78.1250, Loss: 1.8047\n",
            "Epoch [178/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7201\n",
            "Epoch [178/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6432\n",
            "Epoch [178/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7182\n",
            "Epoch [178/200], Step [140/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.4995\n",
            "Epoch [178/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8827\n",
            "Overall accuracy for this epoch:  34.972133757961785\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [179/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8273\n",
            "Epoch [179/200], Step [10/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 2.0226\n",
            "Epoch [179/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8616\n",
            "Epoch [179/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.5350\n",
            "Epoch [179/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7960\n",
            "Epoch [179/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9146\n",
            "Epoch [179/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7665\n",
            "Epoch [179/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.5904\n",
            "Epoch [179/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.3185\n",
            "Epoch [179/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.5392\n",
            "Epoch [179/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8555\n",
            "Epoch [179/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8771\n",
            "Epoch [179/200], Step [120/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 75.0000, Loss: 1.7852\n",
            "Epoch [179/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7104\n",
            "Epoch [179/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7018\n",
            "Epoch [179/200], Step [150/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.5452\n",
            "Overall accuracy for this epoch:  34.972133757961785\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [180/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8459\n",
            "Epoch [180/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6870\n",
            "Epoch [180/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.9058\n",
            "Epoch [180/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7442\n",
            "Epoch [180/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5496\n",
            "Epoch [180/200], Step [50/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6912\n",
            "Epoch [180/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 96.8750, Loss: 1.7067\n",
            "Epoch [180/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9567\n",
            "Epoch [180/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.5994\n",
            "Epoch [180/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 75.0000, Loss: 1.7355\n",
            "Epoch [180/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9635\n",
            "Epoch [180/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.8456\n",
            "Epoch [180/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 90.6250, Loss: 1.8013\n",
            "Epoch [180/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.9493\n",
            "Epoch [180/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8266\n",
            "Epoch [180/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8943\n",
            "Overall accuracy for this epoch:  35.42993630573248\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [181/200], Step [0/157], Time (s): 2.2, Acc1: 18.7500, Acc5: 75.0000, Loss: 1.9409\n",
            "Epoch [181/200], Step [10/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8695\n",
            "Epoch [181/200], Step [20/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.0532\n",
            "Epoch [181/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7649\n",
            "Epoch [181/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8200\n",
            "Epoch [181/200], Step [50/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.9698\n",
            "Epoch [181/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8509\n",
            "Epoch [181/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7383\n",
            "Epoch [181/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8097\n",
            "Epoch [181/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.8115\n",
            "Epoch [181/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6708\n",
            "Epoch [181/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9067\n",
            "Epoch [181/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.8991\n",
            "Epoch [181/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8953\n",
            "Epoch [181/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.4680\n",
            "Epoch [181/200], Step [150/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.5398\n",
            "Overall accuracy for this epoch:  34.892515923566876\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [182/200], Step [0/157], Time (s): 2.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.4482\n",
            "Epoch [182/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7891\n",
            "Epoch [182/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6207\n",
            "Epoch [182/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6893\n",
            "Epoch [182/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8285\n",
            "Epoch [182/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7910\n",
            "Epoch [182/200], Step [60/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.6315\n",
            "Epoch [182/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7914\n",
            "Epoch [182/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 2.0083\n",
            "Epoch [182/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6320\n",
            "Epoch [182/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6619\n",
            "Epoch [182/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6332\n",
            "Epoch [182/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8546\n",
            "Epoch [182/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7399\n",
            "Epoch [182/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 2.0738\n",
            "Epoch [182/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7919\n",
            "Overall accuracy for this epoch:  34.63375796178344\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [183/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.7004\n",
            "Epoch [183/200], Step [10/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5240\n",
            "Epoch [183/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.8119\n",
            "Epoch [183/200], Step [30/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.5912\n",
            "Epoch [183/200], Step [40/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.6870\n",
            "Epoch [183/200], Step [50/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9087\n",
            "Epoch [183/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7641\n",
            "Epoch [183/200], Step [70/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 87.5000, Loss: 2.1247\n",
            "Epoch [183/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7008\n",
            "Epoch [183/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7242\n",
            "Epoch [183/200], Step [100/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 96.8750, Loss: 1.3298\n",
            "Epoch [183/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.8438\n",
            "Epoch [183/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9381\n",
            "Epoch [183/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5459\n",
            "Epoch [183/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.7981\n",
            "Epoch [183/200], Step [150/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.3868\n",
            "Overall accuracy for this epoch:  35.2906050955414\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [184/200], Step [0/157], Time (s): 2.0, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.8480\n",
            "Epoch [184/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8442\n",
            "Epoch [184/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.9324\n",
            "Epoch [184/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8362\n",
            "Epoch [184/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7537\n",
            "Epoch [184/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 100.0000, Loss: 1.4380\n",
            "Epoch [184/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 2.2164\n",
            "Epoch [184/200], Step [70/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6381\n",
            "Epoch [184/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 2.0549\n",
            "Epoch [184/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7504\n",
            "Epoch [184/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8891\n",
            "Epoch [184/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5771\n",
            "Epoch [184/200], Step [120/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8634\n",
            "Epoch [184/200], Step [130/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.5503\n",
            "Epoch [184/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6599\n",
            "Epoch [184/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6441\n",
            "Overall accuracy for this epoch:  35.2109872611465\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [185/200], Step [0/157], Time (s): 2.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.9142\n",
            "Epoch [185/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7888\n",
            "Epoch [185/200], Step [20/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9010\n",
            "Epoch [185/200], Step [30/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.4980\n",
            "Epoch [185/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6938\n",
            "Epoch [185/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.8265\n",
            "Epoch [185/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.8825\n",
            "Epoch [185/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.1384\n",
            "Epoch [185/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9168\n",
            "Epoch [185/200], Step [90/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9788\n",
            "Epoch [185/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9286\n",
            "Epoch [185/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 2.0178\n",
            "Epoch [185/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9136\n",
            "Epoch [185/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9083\n",
            "Epoch [185/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7128\n",
            "Epoch [185/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.8232\n",
            "Overall accuracy for this epoch:  35.05175159235669\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [186/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6948\n",
            "Epoch [186/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 93.7500, Loss: 1.7662\n",
            "Epoch [186/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9963\n",
            "Epoch [186/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 71.8750, Loss: 1.8917\n",
            "Epoch [186/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7016\n",
            "Epoch [186/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.5740\n",
            "Epoch [186/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7062\n",
            "Epoch [186/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6098\n",
            "Epoch [186/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8387\n",
            "Epoch [186/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.7262\n",
            "Epoch [186/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6430\n",
            "Epoch [186/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 2.0186\n",
            "Epoch [186/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6762\n",
            "Epoch [186/200], Step [130/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7685\n",
            "Epoch [186/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8915\n",
            "Epoch [186/200], Step [150/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5821\n",
            "Overall accuracy for this epoch:  35.2109872611465\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [187/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 96.8750, Loss: 1.5244\n",
            "Epoch [187/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6986\n",
            "Epoch [187/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5824\n",
            "Epoch [187/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8508\n",
            "Epoch [187/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9723\n",
            "Epoch [187/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6905\n",
            "Epoch [187/200], Step [60/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8342\n",
            "Epoch [187/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.5936\n",
            "Epoch [187/200], Step [80/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 93.7500, Loss: 1.4591\n",
            "Epoch [187/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8013\n",
            "Epoch [187/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7272\n",
            "Epoch [187/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8132\n",
            "Epoch [187/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.5750\n",
            "Epoch [187/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7293\n",
            "Epoch [187/200], Step [140/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.6315\n",
            "Epoch [187/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.8578\n",
            "Overall accuracy for this epoch:  34.87261146496815\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [188/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8082\n",
            "Epoch [188/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9428\n",
            "Epoch [188/200], Step [20/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.7085\n",
            "Epoch [188/200], Step [30/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.9013\n",
            "Epoch [188/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7624\n",
            "Epoch [188/200], Step [50/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.6967\n",
            "Epoch [188/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.7479\n",
            "Epoch [188/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5021\n",
            "Epoch [188/200], Step [80/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 68.7500, Loss: 2.0791\n",
            "Epoch [188/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6651\n",
            "Epoch [188/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.6301\n",
            "Epoch [188/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7169\n",
            "Epoch [188/200], Step [120/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.5938\n",
            "Epoch [188/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.9611\n",
            "Epoch [188/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7483\n",
            "Epoch [188/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6700\n",
            "Overall accuracy for this epoch:  35.01194267515923\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [189/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 2.0811\n",
            "Epoch [189/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.8320\n",
            "Epoch [189/200], Step [20/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.5589\n",
            "Epoch [189/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 71.8750, Loss: 1.9946\n",
            "Epoch [189/200], Step [40/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8552\n",
            "Epoch [189/200], Step [50/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9599\n",
            "Epoch [189/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.9463\n",
            "Epoch [189/200], Step [70/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6433\n",
            "Epoch [189/200], Step [80/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7983\n",
            "Epoch [189/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.7059\n",
            "Epoch [189/200], Step [100/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.7125\n",
            "Epoch [189/200], Step [110/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 71.8750, Loss: 1.8920\n",
            "Epoch [189/200], Step [120/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6930\n",
            "Epoch [189/200], Step [130/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7647\n",
            "Epoch [189/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.6118\n",
            "Epoch [189/200], Step [150/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 87.5000, Loss: 1.9181\n",
            "Overall accuracy for this epoch:  34.79299363057325\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [190/200], Step [0/157], Time (s): 2.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.6679\n",
            "Epoch [190/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6071\n",
            "Epoch [190/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8291\n",
            "Epoch [190/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.9346\n",
            "Epoch [190/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7471\n",
            "Epoch [190/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 2.0247\n",
            "Epoch [190/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8411\n",
            "Epoch [190/200], Step [70/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7680\n",
            "Epoch [190/200], Step [80/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 2.1090\n",
            "Epoch [190/200], Step [90/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 96.8750, Loss: 1.4791\n",
            "Epoch [190/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.6459\n",
            "Epoch [190/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7227\n",
            "Epoch [190/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9618\n",
            "Epoch [190/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7641\n",
            "Epoch [190/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.5341\n",
            "Epoch [190/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.9701\n",
            "Overall accuracy for this epoch:  34.693471337579616\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [191/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.6867\n",
            "Epoch [191/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7801\n",
            "Epoch [191/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6702\n",
            "Epoch [191/200], Step [30/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 78.1250, Loss: 2.0834\n",
            "Epoch [191/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6697\n",
            "Epoch [191/200], Step [50/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7821\n",
            "Epoch [191/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6804\n",
            "Epoch [191/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 1.9622\n",
            "Epoch [191/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9975\n",
            "Epoch [191/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.5914\n",
            "Epoch [191/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.6579\n",
            "Epoch [191/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.8143\n",
            "Epoch [191/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.9425\n",
            "Epoch [191/200], Step [130/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 84.3750, Loss: 1.7785\n",
            "Epoch [191/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.5844\n",
            "Epoch [191/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.8681\n",
            "Overall accuracy for this epoch:  34.414808917197455\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [192/200], Step [0/157], Time (s): 2.2, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6637\n",
            "Epoch [192/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.7228\n",
            "Epoch [192/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.8462\n",
            "Epoch [192/200], Step [30/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9300\n",
            "Epoch [192/200], Step [40/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.8199\n",
            "Epoch [192/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.7297\n",
            "Epoch [192/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.9071\n",
            "Epoch [192/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6428\n",
            "Epoch [192/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.5576\n",
            "Epoch [192/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.5814\n",
            "Epoch [192/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.8184\n",
            "Epoch [192/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7697\n",
            "Epoch [192/200], Step [120/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6264\n",
            "Epoch [192/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5599\n",
            "Epoch [192/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7241\n",
            "Epoch [192/200], Step [150/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 90.6250, Loss: 1.7272\n",
            "Overall accuracy for this epoch:  35.6687898089172\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [193/200], Step [0/157], Time (s): 2.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 2.0673\n",
            "Epoch [193/200], Step [10/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.7738\n",
            "Epoch [193/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6768\n",
            "Epoch [193/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7700\n",
            "Epoch [193/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.9297\n",
            "Epoch [193/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5859\n",
            "Epoch [193/200], Step [60/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7490\n",
            "Epoch [193/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8922\n",
            "Epoch [193/200], Step [80/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6586\n",
            "Epoch [193/200], Step [90/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.7300\n",
            "Epoch [193/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.7704\n",
            "Epoch [193/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.9012\n",
            "Epoch [193/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 2.0071\n",
            "Epoch [193/200], Step [130/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.8751\n",
            "Epoch [193/200], Step [140/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 93.7500, Loss: 1.8065\n",
            "Epoch [193/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6027\n",
            "Overall accuracy for this epoch:  35.64888535031847\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [194/200], Step [0/157], Time (s): 2.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.9520\n",
            "Epoch [194/200], Step [10/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.6125\n",
            "Epoch [194/200], Step [20/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.5925\n",
            "Epoch [194/200], Step [30/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.8617\n",
            "Epoch [194/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 1.7007\n",
            "Epoch [194/200], Step [50/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 90.6250, Loss: 1.8011\n",
            "Epoch [194/200], Step [60/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.7001\n",
            "Epoch [194/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 81.2500, Loss: 1.7777\n",
            "Epoch [194/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 75.0000, Loss: 1.8185\n",
            "Epoch [194/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.6134\n",
            "Epoch [194/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 71.8750, Loss: 2.0900\n",
            "Epoch [194/200], Step [110/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7131\n",
            "Epoch [194/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.8571\n",
            "Epoch [194/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6732\n",
            "Epoch [194/200], Step [140/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7595\n",
            "Epoch [194/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8000\n",
            "Overall accuracy for this epoch:  35.25079617834395\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [195/200], Step [0/157], Time (s): 2.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8934\n",
            "Epoch [195/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.7202\n",
            "Epoch [195/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6159\n",
            "Epoch [195/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.7847\n",
            "Epoch [195/200], Step [40/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6653\n",
            "Epoch [195/200], Step [50/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 1.8554\n",
            "Epoch [195/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6817\n",
            "Epoch [195/200], Step [70/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.7329\n",
            "Epoch [195/200], Step [80/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 84.3750, Loss: 1.7995\n",
            "Epoch [195/200], Step [90/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9948\n",
            "Epoch [195/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 2.0109\n",
            "Epoch [195/200], Step [110/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.8992\n",
            "Epoch [195/200], Step [120/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 87.5000, Loss: 1.8481\n",
            "Epoch [195/200], Step [130/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7670\n",
            "Epoch [195/200], Step [140/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 93.7500, Loss: 1.6425\n",
            "Epoch [195/200], Step [150/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 1.7030\n",
            "Overall accuracy for this epoch:  34.71337579617835\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [196/200], Step [0/157], Time (s): 2.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7310\n",
            "Epoch [196/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6679\n",
            "Epoch [196/200], Step [20/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7407\n",
            "Epoch [196/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 93.7500, Loss: 1.6183\n",
            "Epoch [196/200], Step [40/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 84.3750, Loss: 1.7231\n",
            "Epoch [196/200], Step [50/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 93.7500, Loss: 1.4913\n",
            "Epoch [196/200], Step [60/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 81.2500, Loss: 1.8859\n",
            "Epoch [196/200], Step [70/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.8448\n",
            "Epoch [196/200], Step [80/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8443\n",
            "Epoch [196/200], Step [90/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 96.8750, Loss: 1.6785\n",
            "Epoch [196/200], Step [100/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7411\n",
            "Epoch [196/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6558\n",
            "Epoch [196/200], Step [120/157], Time (s): 1.1, Acc1: 56.2500, Acc5: 90.6250, Loss: 1.4706\n",
            "Epoch [196/200], Step [130/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7487\n",
            "Epoch [196/200], Step [140/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.9028\n",
            "Epoch [196/200], Step [150/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8775\n",
            "Overall accuracy for this epoch:  34.693471337579616\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [197/200], Step [0/157], Time (s): 2.2, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7876\n",
            "Epoch [197/200], Step [10/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 81.2500, Loss: 1.9149\n",
            "Epoch [197/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 2.0393\n",
            "Epoch [197/200], Step [30/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 81.2500, Loss: 1.7417\n",
            "Epoch [197/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 87.5000, Loss: 1.6741\n",
            "Epoch [197/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.6606\n",
            "Epoch [197/200], Step [60/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 71.8750, Loss: 1.9830\n",
            "Epoch [197/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9018\n",
            "Epoch [197/200], Step [80/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 75.0000, Loss: 1.9243\n",
            "Epoch [197/200], Step [90/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 78.1250, Loss: 1.8322\n",
            "Epoch [197/200], Step [100/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.7998\n",
            "Epoch [197/200], Step [110/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7292\n",
            "Epoch [197/200], Step [120/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.8240\n",
            "Epoch [197/200], Step [130/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 2.0276\n",
            "Epoch [197/200], Step [140/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.7065\n",
            "Epoch [197/200], Step [150/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 2.0615\n",
            "Overall accuracy for this epoch:  35.50955414012739\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [198/200], Step [0/157], Time (s): 2.1, Acc1: 18.7500, Acc5: 71.8750, Loss: 2.1895\n",
            "Epoch [198/200], Step [10/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6636\n",
            "Epoch [198/200], Step [20/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 1.7635\n",
            "Epoch [198/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8974\n",
            "Epoch [198/200], Step [40/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 75.0000, Loss: 1.9600\n",
            "Epoch [198/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7141\n",
            "Epoch [198/200], Step [60/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7350\n",
            "Epoch [198/200], Step [70/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.7220\n",
            "Epoch [198/200], Step [80/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.8303\n",
            "Epoch [198/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.5893\n",
            "Epoch [198/200], Step [100/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.7948\n",
            "Epoch [198/200], Step [110/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.6955\n",
            "Epoch [198/200], Step [120/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 75.0000, Loss: 1.9090\n",
            "Epoch [198/200], Step [130/157], Time (s): 1.2, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.6642\n",
            "Epoch [198/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 84.3750, Loss: 1.6859\n",
            "Epoch [198/200], Step [150/157], Time (s): 1.1, Acc1: 15.6250, Acc5: 75.0000, Loss: 2.1718\n",
            "Overall accuracy for this epoch:  34.93232484076433\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [199/200], Step [0/157], Time (s): 2.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 1.8242\n",
            "Epoch [199/200], Step [10/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 90.6250, Loss: 1.6331\n",
            "Epoch [199/200], Step [20/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 1.6227\n",
            "Epoch [199/200], Step [30/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 90.6250, Loss: 1.6596\n",
            "Epoch [199/200], Step [40/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 90.6250, Loss: 1.8560\n",
            "Epoch [199/200], Step [50/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.8827\n",
            "Epoch [199/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 1.8735\n",
            "Epoch [199/200], Step [70/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 90.6250, Loss: 1.5872\n",
            "Epoch [199/200], Step [80/157], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.6980\n",
            "Epoch [199/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.6291\n",
            "Epoch [199/200], Step [100/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8656\n",
            "Epoch [199/200], Step [110/157], Time (s): 1.1, Acc1: 40.6250, Acc5: 96.8750, Loss: 1.5277\n",
            "Epoch [199/200], Step [120/157], Time (s): 1.1, Acc1: 50.0000, Acc5: 87.5000, Loss: 1.5964\n",
            "Epoch [199/200], Step [130/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 81.2500, Loss: 1.9874\n",
            "Epoch [199/200], Step [140/157], Time (s): 1.1, Acc1: 21.8750, Acc5: 93.7500, Loss: 1.8716\n",
            "Epoch [199/200], Step [150/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 1.9783\n",
            "Overall accuracy for this epoch:  34.056528662420384\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Epoch [200/200], Step [0/157], Time (s): 2.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.7673\n",
            "Epoch [200/200], Step [10/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 1.7034\n",
            "Epoch [200/200], Step [20/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 81.2500, Loss: 1.8344\n",
            "Epoch [200/200], Step [30/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 87.5000, Loss: 1.8170\n",
            "Epoch [200/200], Step [40/157], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.7023\n",
            "Epoch [200/200], Step [50/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 93.7500, Loss: 1.6061\n",
            "Epoch [200/200], Step [60/157], Time (s): 1.1, Acc1: 28.1250, Acc5: 78.1250, Loss: 2.0677\n",
            "Epoch [200/200], Step [70/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 75.0000, Loss: 1.9013\n",
            "Epoch [200/200], Step [80/157], Time (s): 1.1, Acc1: 18.7500, Acc5: 75.0000, Loss: 1.9861\n",
            "Epoch [200/200], Step [90/157], Time (s): 1.1, Acc1: 31.2500, Acc5: 93.7500, Loss: 1.7693\n",
            "Epoch [200/200], Step [100/157], Time (s): 1.1, Acc1: 59.3750, Acc5: 93.7500, Loss: 1.4132\n",
            "Epoch [200/200], Step [110/157], Time (s): 1.1, Acc1: 25.0000, Acc5: 90.6250, Loss: 1.7857\n",
            "Epoch [200/200], Step [120/157], Time (s): 1.1, Acc1: 53.1250, Acc5: 87.5000, Loss: 1.6272\n",
            "Epoch [200/200], Step [130/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.7602\n",
            "Epoch [200/200], Step [140/157], Time (s): 1.1, Acc1: 46.8750, Acc5: 87.5000, Loss: 1.6066\n",
            "Epoch [200/200], Step [150/157], Time (s): 1.1, Acc1: 34.3750, Acc5: 93.7500, Loss: 1.6969\n",
            "Overall accuracy for this epoch:  35.6687898089172\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "Step [0/250], Time (s): 0.9, Acc1: 28.1250, Acc5: 84.3750, Loss: 1.9376\n",
            "Step [10/250], Time (s): 1.1, Acc1: 43.7500, Acc5: 87.5000, Loss: 1.6074\n",
            "Step [20/250], Time (s): 1.1, Acc1: 34.3750, Acc5: 84.3750, Loss: 1.9406\n",
            "Step [30/250], Time (s): 1.1, Acc1: 46.8750, Acc5: 96.8750, Loss: 1.6951\n",
            "Step [40/250], Time (s): 1.1, Acc1: 34.3750, Acc5: 78.1250, Loss: 2.2987\n",
            "Step [50/250], Time (s): 1.1, Acc1: 40.6250, Acc5: 81.2500, Loss: 1.6011\n",
            "Step [60/250], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 2.0047\n",
            "Step [70/250], Time (s): 1.1, Acc1: 40.6250, Acc5: 84.3750, Loss: 2.1548\n",
            "Step [80/250], Time (s): 1.1, Acc1: 37.5000, Acc5: 84.3750, Loss: 1.9621\n",
            "Step [90/250], Time (s): 1.1, Acc1: 37.5000, Acc5: 90.6250, Loss: 1.8767\n",
            "Step [100/250], Time (s): 1.1, Acc1: 40.6250, Acc5: 87.5000, Loss: 2.0350\n",
            "Step [110/250], Time (s): 1.1, Acc1: 31.2500, Acc5: 78.1250, Loss: 2.1063\n",
            "Step [120/250], Time (s): 1.1, Acc1: 31.2500, Acc5: 90.6250, Loss: 2.1160\n",
            "Step [130/250], Time (s): 1.1, Acc1: 25.0000, Acc5: 87.5000, Loss: 1.8112\n",
            "Step [140/250], Time (s): 1.1, Acc1: 31.2500, Acc5: 96.8750, Loss: 1.5974\n",
            "Step [150/250], Time (s): 1.1, Acc1: 18.7500, Acc5: 81.2500, Loss: 2.5556\n",
            "Step [160/250], Time (s): 1.1, Acc1: 53.1250, Acc5: 81.2500, Loss: 2.0451\n",
            "Step [170/250], Time (s): 1.1, Acc1: 46.8750, Acc5: 71.8750, Loss: 2.1923\n",
            "Step [180/250], Time (s): 1.1, Acc1: 37.5000, Acc5: 81.2500, Loss: 1.8789\n",
            "Step [190/250], Time (s): 1.1, Acc1: 37.5000, Acc5: 87.5000, Loss: 1.8503\n",
            "Step [200/250], Time (s): 1.1, Acc1: 43.7500, Acc5: 84.3750, Loss: 1.7523\n",
            "Step [210/250], Time (s): 1.1, Acc1: 28.1250, Acc5: 84.3750, Loss: 2.2796\n",
            "Step [220/250], Time (s): 1.1, Acc1: 31.2500, Acc5: 87.5000, Loss: 2.2988\n",
            "Step [230/250], Time (s): 1.1, Acc1: 28.1250, Acc5: 93.7500, Loss: 2.1002\n",
            "Step [240/250], Time (s): 1.1, Acc1: 18.7500, Acc5: 84.3750, Loss: 2.2546\n",
            "Testing Accuracy:  35.2125\n",
            "Saving model and log-file to ./logs/linear_model/Sun May 10 06:38:01 2020\n",
            "not enough models there yet, nothing to delete\n",
            "not enough models there yet, nothing to delete\n",
            "2640404480it [1:08:36, 641365.96it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9H7SBlHOA6I",
        "colab_type": "text"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vbOkPIVEsVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, save_dir=''):\n",
        "  trainloader = DataLoader(stl10_train, batch_size=64, shuffle=True)\n",
        "  testloader = DataLoader(stl10_test, batch_size=64, shuffle=True)\n",
        "  \n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "\n",
        "  best_acc = 0\n",
        "\n",
        "  save_dir = osp.join(\"/content/drive/My Drive/Gradient-isolated GNN/4x4/\", save_dir)\n",
        "  if not osp.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  checkpoint = \"acc_{:.4f}--epochs_{}--batch_size_64--dim_9216_512_64_10--optim_Adam_lr_0.003--F_nll_loss.pt\"\n",
        "\n",
        "  epochs = 200\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    train_acc = 0\n",
        "    model.train()\n",
        "    for batch in trainloader:\n",
        "      optimizer.zero_grad()\n",
        "      batch = batch.to(device)\n",
        "      emb, logits = model(batch)\n",
        "      labels = batch.y\n",
        "      loss = model.loss(logits, labels)\n",
        "      pred = logits.argmax(dim=1) \n",
        "\n",
        "      train_acc += pred.eq(labels).sum().item()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_acc /= len(trainloader.dataset)\n",
        "    running_loss /= len(trainloader.dataset)\n",
        "    \n",
        "    # cheating validation = test\n",
        "    \n",
        "    test_acc = test(model, testloader)\n",
        "    if (test_acc >= best_acc):\n",
        "      best_acc = test_acc\n",
        "\n",
        "    if e % 5 == 0:\n",
        "      print(\"Epoch {}/{}. Loss: {:.4f}. Train accuracy: {:.4f}. Test accuracy: {:.4f}\".format(e+1, epochs, running_loss, train_acc, test_acc))\n",
        "\n",
        "    if e == epochs - 1:\n",
        "      print(\"Epoch {}/{}. Loss: {:.4f}. Train accuracy: {:.4f}. Test accuracy: {:.4f}\".format(e+1, epochs, running_loss, train_acc, test_acc))\n",
        "      torch.save(model.state_dict(), osp.join(save_dir,checkpoint.format(test_acc, e+1)))\n",
        "      print(\"Best accuracy: {:.4f}. Last accuracy: {:.4f}\".format(best_acc, test_acc))       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ4NdLoakAqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, testloader):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  accuracy = 0\n",
        "  model.eval()\n",
        "  for batch in testloader:\n",
        "    with torch.no_grad():\n",
        "      batch = batch.to(device)\n",
        "      emb, logits = model(batch)\n",
        "      pred = logits.argmax(dim=1)\n",
        "      labels = batch.y\n",
        "    \n",
        "    accuracy += pred.eq(labels).sum().item()\n",
        "\n",
        "  accuracy /= len(testloader.dataset)\n",
        "  return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW9M-UZ2pxY7",
        "colab_type": "text"
      },
      "source": [
        "# Create a Custom Graph Dataset from STL10\n",
        "Each image is divided into a $2\\times2$ grid. Each patch of the grid is a node of the graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXM0yhHgqPZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GraphSTL10(InMemoryDataset):\n",
        "  def __init__(self, root, split):\n",
        "    # !python -m GreedyInfoMax.vision.modded_downstream_classification \\\n",
        "    # --model_path \"/content/drive/My Drive/Greedy_InfoMax/logs/modded_vision_experiment_module_all\" \\\n",
        "    # --model_num 5 \\\n",
        "    # --download_dataset\n",
        "    self.opt = arg_parser.parse_args()\n",
        "    self.opt.download_dataset=True\n",
        "    self.opt.model_path = \"/content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\"\n",
        "    self.opt.model_num = 2\n",
        "    self.opt.grid_dims = 4 \n",
        "    \n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.context_models = []\n",
        "    for pid in range(self.opt.grid_dims * self.opt.grid_dims):\n",
        "      context_model, _ = load_vision_model.load_model_and_optimizer(\n",
        "                self.opt, reload_model=True, calc_loss=False, patch_idx=pid)\n",
        "      context_model.module.switch_calc_loss(False)\n",
        "      for param in context_model.parameters():\n",
        "        param.requires_grad = False\n",
        "      context_model.eval()\n",
        "      context_model.to(self.device)\n",
        "      self.context_models.append(context_model)\n",
        "      \n",
        "    self.dataloader = None\n",
        "\n",
        "    _, _, trainloader, _, testloader, _ = get_dataloader.get_dataloader(self.opt)\n",
        "    self.split = split\n",
        "    if (split == 'train'):\n",
        "      # self.dataset = datasets.STL10(root='/tmp/stl10_train', split='train', download=True)\n",
        "      self.dataloader = trainloader\n",
        "    if (split == 'test'):\n",
        "      # self.dataset = datasets.STL10(root='/tmp/stl10_test', split='test', download=True)\n",
        "       self.dataloader = testloader\n",
        "    \n",
        "    super(GraphSTL10, self).__init__(root)\n",
        "    # self.num_classes = 10\n",
        "    self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return []\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    if (self.split == 'train'):\n",
        "      return ['graphstl10_train.pt']\n",
        "    if (self.split == 'test'):\n",
        "      return ['graphstl10_test.pt']\n",
        "    \n",
        "    return []\n",
        "\n",
        "  def download(self):\n",
        "    pass\n",
        "\n",
        "  def process(self):\n",
        "    \n",
        "    \"\"\"Pytorch ResNet50\n",
        "    # def crop(image,pc_height,pc_width):\n",
        "    #   im_width, im_height = image.size\n",
        "    #   for i in range(im_height//pc_height):\n",
        "    #     for j in range(im_width//pc_width):\n",
        "    #       box = (j*pc_width, i*pc_height, (j+1)*pc_width, (i+1)*pc_height)\n",
        "    #       yield image.crop(box)\n",
        "\n",
        "    # data_list = []\n",
        "\n",
        "    # preprocess = transforms.Compose([transforms.Resize(256),\n",
        "    #                                     transforms.CenterCrop(224),\n",
        "    #                                     transforms.ToTensor(),\n",
        "    #                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    # resnet50 = models.resnet50(pretrained=True)\n",
        "    # resnet50.fc = nn.Identity()\n",
        "\n",
        "    # for param in resnet50.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # resnet50.to(device)\n",
        "\n",
        "    # source_nodes = [i for i in range(0,4) for j in range(0,4)]\n",
        "    # target_nodes = [j for i in range(0,4) for j in range(0,4)]\n",
        "    # edge_index = torch.tensor([source_nodes, target_nodes],dtype = torch.long)\n",
        "\n",
        "    # for imid, (image, label) in enumerate(self.dataset):\n",
        "    #   # neighbors = np.arange(imid, imid+4)\n",
        "    #   # mask = [1]*len(neighbors)\n",
        "    #   im_height, im_width = image.size\n",
        "    #   pc_height, pc_width = im_height//2, im_width//2\n",
        "    #   node_features = []\n",
        "    #   for pid,piece in enumerate(crop(image, pc_height, pc_width)):\n",
        "    #     patch = Image.new('RGB', (pc_width, pc_height), 255)\n",
        "    #     patch.paste(piece)\n",
        "    #     patch = preprocess(patch)\n",
        "    #     patch = patch.view(1,*patch.shape)\n",
        "    #     patch = patch.to(device)\n",
        "    #     patch = resnet50.forward(patch).to(torch.device('cpu'))\n",
        "    #     node_features.append(torch.tensor(patch))\n",
        "    \"\"\"\n",
        "    num_patches = self.opt.grid_dims * self.opt.grid_dims\n",
        "\n",
        "    source_nodes = [i for i in range(0,num_patches) for j in range(0,num_patches)]\n",
        "    target_nodes = [j for i in range(0,num_patches) for j in range(0,num_patches)]\n",
        "    edge_index = torch.tensor([source_nodes, target_nodes],dtype = torch.long)\n",
        "\n",
        "    data_list = []\n",
        "\n",
        "    for bid, (imgs, labels) in enumerate(self.dataloader):\n",
        "      batch_size, num_channels, img_height, img_width = imgs.shape\n",
        "      if not(batch_size == self.opt.batch_size_multiGPU):\n",
        "        continue\n",
        "      patches = []\n",
        "      node_features = []\n",
        "      \n",
        "      step_h = img_height // self.opt.grid_dims\n",
        "      step_w = img_width // self.opt.grid_dims\n",
        "      for height in range(0, img_height, step_h):\n",
        "        for width in range(0, img_width, step_w):\n",
        "          patches.append(imgs[:, :, height:height+step_h, width:width+step_w])    \n",
        "\n",
        "      for pid in range(num_patches):\n",
        "        patch = patches[pid].to(self.device)\n",
        "        patch = self.context_models[pid](patch,labels)[2]\n",
        "        patch = patch.to(torch.device('cpu'))\n",
        "        node_features.append(patch)\n",
        "\n",
        "      node_features = torch.stack(node_features)\n",
        "      node_features = node_features.transpose(0,1)\n",
        "      node_features = node_features.view(*node_features.shape[:-3],-1)\n",
        "      # pdb.set_trace()\n",
        "      labels = labels.to(torch.device('cpu'))\n",
        "      for gid in range(batch_size):  \n",
        "        data = Data(x=node_features[gid], edge_index=edge_index.clone(), y=labels[gid].unsqueeze(0))\n",
        "        data_list.append(data)\n",
        "      # pdb.set_trace()\n",
        "    \n",
        "    data, slices = self.collate(data_list)\n",
        "    torch.save((data, slices), self.processed_paths[0])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVAPxv4G_6ia",
        "colab_type": "code",
        "outputId": "1cfed2af-2c72-4193-ec3a-3a45669d1f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stl10_train = GraphSTL10('/graphstl10/',split='train')\n",
        "stl10_test = GraphSTL10('/graphstl10/',split='test')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Use (train+val) / test split\n",
            "Processing...\n",
            "Done!\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Contrasting against  16  negative samples\n",
            "ModuleList(\n",
            "  (0): ModuleList(\n",
            "    (0): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (Conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (1): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "    (2): ResNet_Encoder(\n",
            "      (model): Sequential(\n",
            "        (layer 0): Sequential(\n",
            "          (0): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (shortcut): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "          (1): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): PreActBottleneckNoBN(\n",
            "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (loss): InfoNCE_Loss(\n",
            "        (W_k): ModuleList(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (contrast_loss): ExpNLLLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Loading weights from  /content/drive/My Drive/Greedy_InfoMax/logs/pretrained-resnet34-4x4\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.0.shortcut.0.weight\", \"model.layer 0.0.shortcut.0.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 1, 1]).\n",
            "Error(s) in loading state_dict for ResNet_Encoder:\n",
            "\tMissing key(s) in state_dict: \"model.layer 0.0.conv3.weight\", \"model.layer 0.0.conv3.bias\", \"model.layer 0.1.conv3.weight\", \"model.layer 0.1.conv3.bias\", \"model.layer 0.2.conv3.weight\", \"model.layer 0.2.conv3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"loss.W_k.5.weight\". \n",
            "\tsize mismatch for model.layer 0.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.0.shortcut.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model.layer 0.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for model.layer 0.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.3.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "\tsize mismatch for loss.W_k.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Use (train+val) / test split\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0VbJFx1t3U4",
        "colab_type": "text"
      },
      "source": [
        "# Define custom MessagePassing [WIP]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l-qzzdJxAh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomConv(pyg_nn.MessagePassing):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(CustomConv, self).__init__(aggr='add') #'add' aggregation\n",
        "    self.lin = nn.Linear(in_channels, out_channels)\n",
        "    self.lin_self = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    # x has shape [N, in_channels]\n",
        "    # edge_index has shape [2, E]\n",
        "\n",
        "    # Transform node feature matrix\n",
        "    self_x = self.lin_self(x)\n",
        "    # x = self.lin(x)\n",
        "\n",
        "    return self_x + self.propagate(edge_index, size=(x.size(0), x.size(0)), x=self.lin(x))\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, size):\n",
        "    # Compute messages\n",
        "    # x_j has shape [E, out_channels]\n",
        "    # TODO:\n",
        "    row, col = edge_index\n",
        "    deg = pyg_utils.degree(row, size[0], dtype=x_j.dtype)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "    return x_j\n",
        "\n",
        "  def update(self, aggr_out):\n",
        "    # aggr_out has shape [N, out_channels]\n",
        "    return aggr_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FisgdFj8uByD",
        "colab_type": "text"
      },
      "source": [
        "# Define the Graph Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGgJDe67NZlE",
        "colab_type": "text"
      },
      "source": [
        "## Graph Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stn49Kp4uFf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GCN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(GCN, self).__init__()\n",
        "        \n",
        "    self.dropout = 0.25\n",
        "    self.num_layers = 2\n",
        "    self.hidden = [input_dim, 512, hidden_dim]\n",
        "    # self.resnet = models.resnet50(pretrained=True)\n",
        "    # self.resnet.fc = nn.Identity()\n",
        "\n",
        "    self.convs = nn.ModuleList()\n",
        "    self.lns = nn.ModuleList()\n",
        "\n",
        "    for l in range(self.num_layers):\n",
        "      self.convs.append(self.build_conv_model(self.hidden[l], self.hidden[l+1]))\n",
        "      if (l + 1 < self.num_layers):\n",
        "        self.lns.append(nn.LayerNorm(self.hidden[l+1]))\n",
        "\n",
        "    # post-message-passing\n",
        "    self.post_mp = nn.Sequential(\n",
        "        nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25),\n",
        "        nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def build_conv_model(self, input_dim, hidden_dim):\n",
        "      return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
        "      \n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    if data.num_node_features == 0:\n",
        "      x = torch.ones(data.num_nodes, 1)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.convs[i](x, edge_index)\n",
        "      emb = x\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training = self.training)\n",
        "      if not i == self.num_layers - 1:\n",
        "        x = self.lns[i](x)\n",
        "\n",
        "    x = pyg_nn.global_mean_pool(x, data.batch)\n",
        "    # x = pyg_nn.global_add_pool(x, data.batch)\n",
        "\n",
        "    x = self.post_mp(x)\n",
        "\n",
        "    return emb, F.log_softmax(x, dim=1)\n",
        "\n",
        "  def loss(self, pred, label):\n",
        "    # Negative log-likelihood\n",
        "    return F.nll_loss(pred, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCjwU3pJ-3mA",
        "colab_type": "code",
        "outputId": "e2b291c9-63c1-4d62-e7a6-b4e1f2d43bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "source": [
        "model = GCN(input_dim=stl10_train.num_node_features, hidden_dim=64, output_dim = 10)\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.003)\n",
        "train(model, optimizer)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200. Loss: 0.0362. Train accuracy: 0.1006. Test accuracy: 0.0703\n",
            "Epoch 6/200. Loss: 0.0360. Train accuracy: 0.1012. Test accuracy: 0.1000\n",
            "Epoch 11/200. Loss: 0.0360. Train accuracy: 0.0988. Test accuracy: 0.0999\n",
            "Epoch 16/200. Loss: 0.0360. Train accuracy: 0.0923. Test accuracy: 0.0844\n",
            "Epoch 21/200. Loss: 0.0360. Train accuracy: 0.0958. Test accuracy: 0.0634\n",
            "Epoch 26/200. Loss: 0.0360. Train accuracy: 0.0948. Test accuracy: 0.1000\n",
            "Epoch 31/200. Loss: 0.0360. Train accuracy: 0.0921. Test accuracy: 0.0850\n",
            "Epoch 36/200. Loss: 0.0360. Train accuracy: 0.0948. Test accuracy: 0.1000\n",
            "Epoch 41/200. Loss: 0.0360. Train accuracy: 0.0952. Test accuracy: 0.1000\n",
            "Epoch 46/200. Loss: 0.0360. Train accuracy: 0.0855. Test accuracy: 0.1000\n",
            "Epoch 51/200. Loss: 0.0360. Train accuracy: 0.0929. Test accuracy: 0.1000\n",
            "Epoch 56/200. Loss: 0.0360. Train accuracy: 0.0992. Test accuracy: 0.1000\n",
            "Epoch 61/200. Loss: 0.0360. Train accuracy: 0.0869. Test accuracy: 0.1000\n",
            "Epoch 66/200. Loss: 0.0360. Train accuracy: 0.0968. Test accuracy: 0.1000\n",
            "Epoch 71/200. Loss: 0.0360. Train accuracy: 0.0909. Test accuracy: 0.1000\n",
            "Epoch 76/200. Loss: 0.0360. Train accuracy: 0.0982. Test accuracy: 0.1000\n",
            "Epoch 81/200. Loss: 0.0360. Train accuracy: 0.1002. Test accuracy: 0.1000\n",
            "Epoch 86/200. Loss: 0.0360. Train accuracy: 0.1024. Test accuracy: 0.1000\n",
            "Epoch 91/200. Loss: 0.0360. Train accuracy: 0.0940. Test accuracy: 0.1000\n",
            "Epoch 96/200. Loss: 0.0360. Train accuracy: 0.0917. Test accuracy: 0.1000\n",
            "Epoch 101/200. Loss: 0.0360. Train accuracy: 0.0825. Test accuracy: 0.1000\n",
            "Epoch 106/200. Loss: 0.0360. Train accuracy: 0.0901. Test accuracy: 0.1000\n",
            "Epoch 111/200. Loss: 0.0360. Train accuracy: 0.0903. Test accuracy: 0.1000\n",
            "Epoch 116/200. Loss: 0.0360. Train accuracy: 0.0921. Test accuracy: 0.1000\n",
            "Epoch 121/200. Loss: 0.0360. Train accuracy: 0.0938. Test accuracy: 0.1000\n",
            "Epoch 126/200. Loss: 0.0360. Train accuracy: 0.0966. Test accuracy: 0.1001\n",
            "Epoch 131/200. Loss: 0.0360. Train accuracy: 0.0954. Test accuracy: 0.0991\n",
            "Epoch 136/200. Loss: 0.0360. Train accuracy: 0.0998. Test accuracy: 0.1000\n",
            "Epoch 141/200. Loss: 0.0360. Train accuracy: 0.0859. Test accuracy: 0.1000\n",
            "Epoch 146/200. Loss: 0.0360. Train accuracy: 0.0933. Test accuracy: 0.1000\n",
            "Epoch 151/200. Loss: 0.0360. Train accuracy: 0.0980. Test accuracy: 0.1000\n",
            "Epoch 156/200. Loss: 0.0360. Train accuracy: 0.0958. Test accuracy: 0.1000\n",
            "Epoch 161/200. Loss: 0.0360. Train accuracy: 0.0952. Test accuracy: 0.1000\n",
            "Epoch 166/200. Loss: 0.0360. Train accuracy: 0.0927. Test accuracy: 0.1040\n",
            "Epoch 171/200. Loss: 0.0360. Train accuracy: 0.0952. Test accuracy: 0.1000\n",
            "Epoch 176/200. Loss: 0.0360. Train accuracy: 0.0913. Test accuracy: 0.0877\n",
            "Epoch 181/200. Loss: 0.0360. Train accuracy: 0.0954. Test accuracy: 0.1000\n",
            "Epoch 186/200. Loss: 0.0360. Train accuracy: 0.0853. Test accuracy: 0.0995\n",
            "Epoch 191/200. Loss: 0.0360. Train accuracy: 0.0968. Test accuracy: 0.1070\n",
            "Epoch 196/200. Loss: 0.0360. Train accuracy: 0.0998. Test accuracy: 0.1000\n",
            "Epoch 200/200. Loss: 0.0360. Train accuracy: 0.0938. Test accuracy: 0.1000\n",
            "Best accuracy: 0.1242. Last accuracy: 0.1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLkdsJTNlV7",
        "colab_type": "text"
      },
      "source": [
        "## Graph Attention Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEbtFRWfM5pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(GAT, self).__init__()\n",
        "        \n",
        "    self.dropout = 0.25\n",
        "    self.num_layers = 2\n",
        "    self.hidden = [input_dim, 512, hidden_dim]\n",
        "    # self.resnet = models.resnet50(pretrained=True)\n",
        "    # self.resnet.fc = nn.Identity()\n",
        "\n",
        "    self.convs = nn.ModuleList()\n",
        "    self.lns = nn.ModuleList()\n",
        "\n",
        "    for l in range(self.num_layers):\n",
        "      self.convs.append(self.build_conv_model(self.hidden[l], self.hidden[l+1]))\n",
        "      if (l + 1 < self.num_layers):\n",
        "        self.lns.append(nn.LayerNorm(self.hidden[l+1]))\n",
        "\n",
        "    # post-message-passing\n",
        "    self.post_mp = nn.Sequential(\n",
        "        nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25),\n",
        "        nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def build_conv_model(self, input_dim, hidden_dim):\n",
        "      return pyg_nn.GATConv(input_dim, hidden_dim)\n",
        "      \n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    if data.num_node_features == 0:\n",
        "      x = torch.ones(data.num_nodes, 1)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.convs[i](x, edge_index)\n",
        "      emb = x\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training = self.training)\n",
        "      if not i == self.num_layers - 1:\n",
        "        x = self.lns[i](x)\n",
        "\n",
        "    x = pyg_nn.global_mean_pool(x, data.batch)\n",
        "    # x = pyg_nn.global_add_pool(x, data.batch)\n",
        "\n",
        "    x = self.post_mp(x)\n",
        "\n",
        "    return emb, F.log_softmax(x, dim=1)\n",
        "\n",
        "  def loss(self, pred, label):\n",
        "    # Negative log-likelihood\n",
        "    return F.nll_loss(pred, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ6XCvnmOd4D",
        "colab_type": "code",
        "outputId": "20a04da5-ce64-44e2-dab0-e405adee5981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "source": [
        "model = GAT(input_dim=stl10_train.num_node_features, hidden_dim=64, output_dim = 10)\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.003)\n",
        "train(model, optimizer, save_dir='ResNet50-GreedyInfoMax-GAT')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200. Loss: 0.0362. Train accuracy: 0.0927. Test accuracy: 0.0996\n",
            "Epoch 6/200. Loss: 0.0360. Train accuracy: 0.0909. Test accuracy: 0.1020\n",
            "Epoch 11/200. Loss: 0.0360. Train accuracy: 0.0948. Test accuracy: 0.1061\n",
            "Epoch 16/200. Loss: 0.0360. Train accuracy: 0.0895. Test accuracy: 0.0985\n",
            "Epoch 21/200. Loss: 0.0360. Train accuracy: 0.0954. Test accuracy: 0.0874\n",
            "Epoch 26/200. Loss: 0.0360. Train accuracy: 0.0907. Test accuracy: 0.1060\n",
            "Epoch 31/200. Loss: 0.0360. Train accuracy: 0.0964. Test accuracy: 0.1000\n",
            "Epoch 36/200. Loss: 0.0360. Train accuracy: 0.0885. Test accuracy: 0.1011\n",
            "Epoch 41/200. Loss: 0.0360. Train accuracy: 0.0964. Test accuracy: 0.0951\n",
            "Epoch 46/200. Loss: 0.0360. Train accuracy: 0.0867. Test accuracy: 0.0824\n",
            "Epoch 51/200. Loss: 0.0360. Train accuracy: 0.0976. Test accuracy: 0.0839\n",
            "Epoch 56/200. Loss: 0.0360. Train accuracy: 0.0867. Test accuracy: 0.0995\n",
            "Epoch 61/200. Loss: 0.0360. Train accuracy: 0.0927. Test accuracy: 0.1000\n",
            "Epoch 66/200. Loss: 0.0360. Train accuracy: 0.0927. Test accuracy: 0.0929\n",
            "Epoch 71/200. Loss: 0.0360. Train accuracy: 0.0929. Test accuracy: 0.0900\n",
            "Epoch 76/200. Loss: 0.0360. Train accuracy: 0.0861. Test accuracy: 0.0884\n",
            "Epoch 81/200. Loss: 0.0360. Train accuracy: 0.0980. Test accuracy: 0.0809\n",
            "Epoch 86/200. Loss: 0.0360. Train accuracy: 0.0931. Test accuracy: 0.0870\n",
            "Epoch 91/200. Loss: 0.0360. Train accuracy: 0.0909. Test accuracy: 0.0999\n",
            "Epoch 96/200. Loss: 0.0360. Train accuracy: 0.0970. Test accuracy: 0.0995\n",
            "Epoch 101/200. Loss: 0.0360. Train accuracy: 0.0968. Test accuracy: 0.1010\n",
            "Epoch 106/200. Loss: 0.0360. Train accuracy: 0.0855. Test accuracy: 0.1025\n",
            "Epoch 111/200. Loss: 0.0360. Train accuracy: 0.0887. Test accuracy: 0.0866\n",
            "Epoch 116/200. Loss: 0.0360. Train accuracy: 0.0907. Test accuracy: 0.1196\n",
            "Epoch 121/200. Loss: 0.0360. Train accuracy: 0.0946. Test accuracy: 0.0929\n",
            "Epoch 126/200. Loss: 0.0360. Train accuracy: 0.0921. Test accuracy: 0.0815\n",
            "Epoch 131/200. Loss: 0.0360. Train accuracy: 0.0952. Test accuracy: 0.1000\n",
            "Epoch 136/200. Loss: 0.0360. Train accuracy: 0.0990. Test accuracy: 0.0877\n",
            "Epoch 141/200. Loss: 0.0360. Train accuracy: 0.0938. Test accuracy: 0.0835\n",
            "Epoch 146/200. Loss: 0.0360. Train accuracy: 0.0946. Test accuracy: 0.1089\n",
            "Epoch 151/200. Loss: 0.0360. Train accuracy: 0.0929. Test accuracy: 0.0999\n",
            "Epoch 156/200. Loss: 0.0360. Train accuracy: 0.0980. Test accuracy: 0.1009\n",
            "Epoch 161/200. Loss: 0.0360. Train accuracy: 0.0938. Test accuracy: 0.1000\n",
            "Epoch 166/200. Loss: 0.0360. Train accuracy: 0.0929. Test accuracy: 0.1015\n",
            "Epoch 171/200. Loss: 0.0360. Train accuracy: 0.0974. Test accuracy: 0.1001\n",
            "Epoch 176/200. Loss: 0.0360. Train accuracy: 0.0938. Test accuracy: 0.1200\n",
            "Epoch 181/200. Loss: 0.0360. Train accuracy: 0.0940. Test accuracy: 0.1216\n",
            "Epoch 186/200. Loss: 0.0360. Train accuracy: 0.0958. Test accuracy: 0.0953\n",
            "Epoch 191/200. Loss: 0.0360. Train accuracy: 0.0964. Test accuracy: 0.1341\n",
            "Epoch 196/200. Loss: 0.0360. Train accuracy: 0.0923. Test accuracy: 0.0943\n",
            "Epoch 200/200. Loss: 0.0360. Train accuracy: 0.0944. Test accuracy: 0.1154\n",
            "Best accuracy: 0.1386. Last accuracy: 0.1154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqt07bM_Nh4q",
        "colab_type": "text"
      },
      "source": [
        "## Attention-based Graph Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3NAzRBfJqAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AGNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(AGNN, self).__init__()\n",
        "        \n",
        "    self.dropout = 0.25\n",
        "    self.num_layers = 2\n",
        "    self.hidden = [input_dim, 512, hidden_dim]\n",
        "    # self.resnet = models.resnet50(pretrained=True)\n",
        "    # self.resnet.fc = nn.Identity()\n",
        "\n",
        "    self.convs = nn.ModuleList()\n",
        "    self.lns = nn.ModuleList()\n",
        "    self.attention = pyg_nn.AGNNConv()\n",
        "\n",
        "    for l in range(self.num_layers):\n",
        "      self.convs.append(self.build_conv_model(self.hidden[l], self.hidden[l+1]))\n",
        "      if (l + 1 < self.num_layers):\n",
        "        self.lns.append(nn.LayerNorm(self.hidden[l+1]))\n",
        "\n",
        "    # post-message-passing\n",
        "    self.post_mp = nn.Sequential(\n",
        "        nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25),\n",
        "        nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def build_conv_model(self, input_dim, hidden_dim):\n",
        "      return nn.Linear(input_dim, hidden_dim)\n",
        "      \n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    if data.num_node_features == 0:\n",
        "      x = torch.ones(data.num_nodes, 1)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.attention(self.convs[i](x), edge_index)\n",
        "      emb = x\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training = self.training)\n",
        "      if not i == self.num_layers - 1:\n",
        "        x = self.lns[i](x)\n",
        "\n",
        "    x = pyg_nn.global_mean_pool(x, data.batch)\n",
        "    # x = pyg_nn.global_add_pool(x, data.batch)\n",
        "\n",
        "    x = self.post_mp(x)\n",
        "\n",
        "    return emb, F.log_softmax(x, dim=1)\n",
        "\n",
        "  def loss(self, pred, label):\n",
        "    # Negative log-likelihood\n",
        "    return F.nll_loss(pred, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I83VKBVKOZOM",
        "colab_type": "code",
        "outputId": "aabb5af3-a070-4b62-9276-f013c3f53267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "source": [
        "model = AGNN(input_dim=stl10_train.num_node_features, hidden_dim=64, output_dim = 10)\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.003)\n",
        "train(model, optimizer, save_dir=\"ResNet50-GreedyInfoMax-AGNN\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200. Loss: 0.0361. Train accuracy: 0.0980. Test accuracy: 0.1000\n",
            "Epoch 6/200. Loss: 0.0360. Train accuracy: 0.0942. Test accuracy: 0.1000\n",
            "Epoch 11/200. Loss: 0.0360. Train accuracy: 0.0946. Test accuracy: 0.1000\n",
            "Epoch 16/200. Loss: 0.0360. Train accuracy: 0.0966. Test accuracy: 0.1000\n",
            "Epoch 21/200. Loss: 0.0360. Train accuracy: 0.0913. Test accuracy: 0.1000\n",
            "Epoch 26/200. Loss: 0.0360. Train accuracy: 0.0962. Test accuracy: 0.1000\n",
            "Epoch 31/200. Loss: 0.0360. Train accuracy: 0.0931. Test accuracy: 0.1000\n",
            "Epoch 36/200. Loss: 0.0360. Train accuracy: 0.0903. Test accuracy: 0.1000\n",
            "Epoch 41/200. Loss: 0.0360. Train accuracy: 0.0877. Test accuracy: 0.1000\n",
            "Epoch 46/200. Loss: 0.0360. Train accuracy: 0.0927. Test accuracy: 0.1000\n",
            "Epoch 51/200. Loss: 0.0360. Train accuracy: 0.0931. Test accuracy: 0.1000\n",
            "Epoch 56/200. Loss: 0.0360. Train accuracy: 0.0940. Test accuracy: 0.1000\n",
            "Epoch 61/200. Loss: 0.0360. Train accuracy: 0.0935. Test accuracy: 0.1000\n",
            "Epoch 66/200. Loss: 0.0360. Train accuracy: 0.0919. Test accuracy: 0.1000\n",
            "Epoch 71/200. Loss: 0.0360. Train accuracy: 0.1004. Test accuracy: 0.1000\n",
            "Epoch 76/200. Loss: 0.0360. Train accuracy: 0.0942. Test accuracy: 0.1003\n",
            "Epoch 81/200. Loss: 0.0360. Train accuracy: 0.1014. Test accuracy: 0.1000\n",
            "Epoch 86/200. Loss: 0.0360. Train accuracy: 0.0923. Test accuracy: 0.1000\n",
            "Epoch 91/200. Loss: 0.0360. Train accuracy: 0.0897. Test accuracy: 0.1000\n",
            "Epoch 96/200. Loss: 0.0360. Train accuracy: 0.0940. Test accuracy: 0.1000\n",
            "Epoch 101/200. Loss: 0.0360. Train accuracy: 0.0946. Test accuracy: 0.1000\n",
            "Epoch 106/200. Loss: 0.0360. Train accuracy: 0.0966. Test accuracy: 0.1000\n",
            "Epoch 111/200. Loss: 0.0360. Train accuracy: 0.0925. Test accuracy: 0.1000\n",
            "Epoch 116/200. Loss: 0.0360. Train accuracy: 0.0917. Test accuracy: 0.1000\n",
            "Epoch 121/200. Loss: 0.0360. Train accuracy: 0.0923. Test accuracy: 0.1000\n",
            "Epoch 126/200. Loss: 0.0360. Train accuracy: 0.0948. Test accuracy: 0.1000\n",
            "Epoch 131/200. Loss: 0.0360. Train accuracy: 0.0903. Test accuracy: 0.1000\n",
            "Epoch 136/200. Loss: 0.0360. Train accuracy: 0.0988. Test accuracy: 0.1000\n",
            "Epoch 141/200. Loss: 0.0360. Train accuracy: 0.1014. Test accuracy: 0.1000\n",
            "Epoch 146/200. Loss: 0.0360. Train accuracy: 0.0935. Test accuracy: 0.1000\n",
            "Epoch 151/200. Loss: 0.0360. Train accuracy: 0.0923. Test accuracy: 0.1000\n",
            "Epoch 156/200. Loss: 0.0360. Train accuracy: 0.1014. Test accuracy: 0.1000\n",
            "Epoch 161/200. Loss: 0.0360. Train accuracy: 0.0948. Test accuracy: 0.1000\n",
            "Epoch 166/200. Loss: 0.0360. Train accuracy: 0.0923. Test accuracy: 0.1000\n",
            "Epoch 171/200. Loss: 0.0360. Train accuracy: 0.0927. Test accuracy: 0.1000\n",
            "Epoch 176/200. Loss: 0.0360. Train accuracy: 0.0893. Test accuracy: 0.1000\n",
            "Epoch 181/200. Loss: 0.0360. Train accuracy: 0.0921. Test accuracy: 0.1000\n",
            "Epoch 186/200. Loss: 0.0360. Train accuracy: 0.0853. Test accuracy: 0.1000\n",
            "Epoch 191/200. Loss: 0.0360. Train accuracy: 0.0909. Test accuracy: 0.1000\n",
            "Epoch 196/200. Loss: 0.0360. Train accuracy: 0.0964. Test accuracy: 0.1000\n",
            "Epoch 200/200. Loss: 0.0360. Train accuracy: 0.0935. Test accuracy: 0.1000\n",
            "Best accuracy: 0.1003. Last accuracy: 0.1000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}